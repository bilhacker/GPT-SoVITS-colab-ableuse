{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bilhacker/GPT-SoVITS-colab-ableuse/blob/main/colab_webui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "环境配置 environment"
      ],
      "metadata": {
        "id": "_o6a8GS2lWQM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9b7iFV3dm1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8a1779d-81a0-41e5-ea2a-621a2a71ea40"
      },
      "source": [
        "!pip install -q condacolab\n",
        "# Setting up condacolab and installing packages\n",
        "import condacolab\n",
        "condacolab.install_from_url(\"https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh\")\n",
        "%cd -q /content\n",
        "!git clone https://github.com/RVC-Boss/GPT-SoVITS\n",
        "!conda install -y -q -c pytorch -c nvidia cudatoolkit\n",
        "%cd -q /content/GPT-SoVITS\n",
        "!conda install -y -q -c conda-forge gcc gxx ffmpeg cmake -c pytorch -c nvidia\n",
        "!/usr/local/bin/pip install -r requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m✨🍰✨ Everything looks OK!\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "fatal: destination path 'GPT-SoVITS' already exists and is not an empty directory.\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Channels:\n",
            " - pytorch\n",
            " - nvidia\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - cudatoolkit\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  cudatoolkit        nvidia/linux-64::cudatoolkit-11.7.0-hd8887f6_10 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  conda                              23.11.0-py39h06a4308_0 --> 24.1.0-py39h06a4308_0 \n",
            "  openssl                                 3.0.12-h7f8727e_0 --> 3.0.13-h7f8727e_0 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... failed\n",
            "\n",
            "CondaVerificationError: The package for cudatoolkit located at /usr/local/pkgs/cudatoolkit-11.7.0-hd8887f6_10\n",
            "appears to be corrupted. The path 'lib/libnppif.so.11.7.3.21'\n",
            "specified in the package manifest cannot be found.\n",
            "\n",
            "SafetyError: The package for cudatoolkit located at /usr/local/pkgs/cudatoolkit-11.7.0-hd8887f6_10\n",
            "appears to be corrupted. The path 'lib/libnppig.so.11.7.3.21'\n",
            "has an incorrect size.\n",
            "  reported size: 32209760 bytes\n",
            "  actual size: 8781824 bytes\n",
            "\n",
            "CondaVerificationError: The package for cudatoolkit located at /usr/local/pkgs/cudatoolkit-11.7.0-hd8887f6_10\n",
            "appears to be corrupted. The path 'lib/libnppist.so.11.7.3.21'\n",
            "specified in the package manifest cannot be found.\n",
            "\n",
            "CondaVerificationError: The package for cudatoolkit located at /usr/local/pkgs/cudatoolkit-11.7.0-hd8887f6_10\n",
            "appears to be corrupted. The path 'lib/libnvToolsExt.so.1'\n",
            "specified in the package manifest cannot be found.\n",
            "\n",
            "\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Channels:\n",
            " - conda-forge\n",
            " - pytorch\n",
            " - nvidia\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - cmake\n",
            "    - ffmpeg\n",
            "    - gcc\n",
            "    - gxx\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _libgcc_mutex-0.1          |      conda_forge           3 KB  conda-forge\n",
            "    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge\n",
            "    aom-3.8.1                  |       h59595ed_0         2.6 MB  conda-forge\n",
            "    binutils_impl_linux-64-2.40|       hf600244_0         5.2 MB  conda-forge\n",
            "    ca-certificates-2024.2.2   |       hbcca054_0         152 KB  conda-forge\n",
            "    cairo-1.18.0               |       h3faef2a_0         959 KB  conda-forge\n",
            "    certifi-2024.2.2           |     pyhd8ed1ab_0         157 KB  conda-forge\n",
            "    cmake-3.28.2               |       hcfe8598_0        17.9 MB  conda-forge\n",
            "    dav1d-1.2.1                |       hd590300_0         742 KB  conda-forge\n",
            "    expat-2.5.0                |       hcb278e6_1         134 KB  conda-forge\n",
            "    ffmpeg-6.1.1               | gpl_h8007c5b_104         9.3 MB  conda-forge\n",
            "    font-ttf-dejavu-sans-mono-2.37|       hab24e00_0         388 KB  conda-forge\n",
            "    font-ttf-inconsolata-3.000 |       h77eed37_0          94 KB  conda-forge\n",
            "    font-ttf-source-code-pro-2.038|       h77eed37_0         684 KB  conda-forge\n",
            "    font-ttf-ubuntu-0.83       |       h77eed37_1         1.5 MB  conda-forge\n",
            "    fontconfig-2.14.2          |       h14ed4e7_0         266 KB  conda-forge\n",
            "    fonts-conda-ecosystem-1    |                0           4 KB  conda-forge\n",
            "    fonts-conda-forge-1        |                0           4 KB  conda-forge\n",
            "    freetype-2.12.1            |       h267a509_2         620 KB  conda-forge\n",
            "    fribidi-1.0.10             |       h36c2ea0_0         112 KB  conda-forge\n",
            "    gcc-13.2.0                 |       h574f8da_2          26 KB  conda-forge\n",
            "    gcc_impl_linux-64-13.2.0   |       h338b0a0_5        50.8 MB  conda-forge\n",
            "    gettext-0.21.1             |       h27087fc_0         4.1 MB  conda-forge\n",
            "    gmp-6.3.0                  |       h59595ed_0         550 KB  conda-forge\n",
            "    gnutls-3.7.9               |       hb077bed_0         1.9 MB  conda-forge\n",
            "    graphite2-1.3.13           |    h58526e2_1001         102 KB  conda-forge\n",
            "    gxx-13.2.0                 |       h574f8da_2          26 KB  conda-forge\n",
            "    gxx_impl_linux-64-13.2.0   |       h338b0a0_5        13.0 MB  conda-forge\n",
            "    harfbuzz-8.3.0             |       h3d44ed6_0         1.5 MB  conda-forge\n",
            "    icu-73.2                   |       h59595ed_0        11.5 MB  conda-forge\n",
            "    kernel-headers_linux-64-2.6.32|      he073ed8_16         692 KB  conda-forge\n",
            "    lame-3.100                 |    h166bdaf_1003         496 KB  conda-forge\n",
            "    ld_impl_linux-64-2.40      |       h41732ed_0         688 KB  conda-forge\n",
            "    libabseil-20230802.1       | cxx17_h59595ed_0         1.2 MB  conda-forge\n",
            "    libarchive-3.6.2           |       h039dbb9_1         824 KB  conda-forge\n",
            "    libass-0.17.1              |       h8fe9dca_1         124 KB  conda-forge\n",
            "    libcurl-8.5.0              |       h251f7ec_0         416 KB\n",
            "    libdrm-2.4.114             |       h166bdaf_0         298 KB  conda-forge\n",
            "    libexpat-2.5.0             |       hcb278e6_1          76 KB  conda-forge\n",
            "    libgcc-devel_linux-64-13.2.0|     ha9c7c90_105         2.5 MB  conda-forge\n",
            "    libgcc-ng-13.2.0           |       h807b86a_5         752 KB  conda-forge\n",
            "    libglib-2.78.3             |       h783c2da_0         2.6 MB  conda-forge\n",
            "    libgomp-13.2.0             |       h807b86a_5         410 KB  conda-forge\n",
            "    libhwloc-2.9.3             |default_h554bfaf_1009         2.5 MB  conda-forge\n",
            "    libiconv-1.17              |       hd590300_2         689 KB  conda-forge\n",
            "    libidn2-2.3.7              |       hd590300_0         124 KB  conda-forge\n",
            "    libopenvino-2023.3.0       |       h2e90f83_0         5.7 MB  conda-forge\n",
            "    libopenvino-auto-batch-plugin-2023.3.0|       hd5fc58b_0         113 KB  conda-forge\n",
            "    libopenvino-auto-plugin-2023.3.0|       hd5fc58b_0         233 KB  conda-forge\n",
            "    libopenvino-hetero-plugin-2023.3.0|       h3ecfda7_0         178 KB  conda-forge\n",
            "    libopenvino-intel-cpu-plugin-2023.3.0|       h2e90f83_0         9.7 MB  conda-forge\n",
            "    libopenvino-intel-gpu-plugin-2023.3.0|       h2e90f83_0         7.8 MB  conda-forge\n",
            "    libopenvino-ir-frontend-2023.3.0|       h3ecfda7_0         194 KB  conda-forge\n",
            "    libopenvino-onnx-frontend-2023.3.0|       hfbc7f12_0         1.5 MB  conda-forge\n",
            "    libopenvino-paddle-frontend-2023.3.0|       hfbc7f12_0         644 KB  conda-forge\n",
            "    libopenvino-pytorch-frontend-2023.3.0|       h59595ed_0         938 KB  conda-forge\n",
            "    libopenvino-tensorflow-frontend-2023.3.0|       h0bff32c_0         1.1 MB  conda-forge\n",
            "    libopenvino-tensorflow-lite-frontend-2023.3.0|       h59595ed_0         446 KB  conda-forge\n",
            "    libopus-1.3.1              |       h7f98852_1         255 KB  conda-forge\n",
            "    libpciaccess-0.17          |       h166bdaf_0          39 KB  conda-forge\n",
            "    libpng-1.6.42              |       h2797004_0         282 KB  conda-forge\n",
            "    libprotobuf-4.25.1         |       hf27288f_1         2.6 MB  conda-forge\n",
            "    libsanitizer-13.2.0        |       h7e041cc_5         3.9 MB  conda-forge\n",
            "    libstdcxx-devel_linux-64-13.2.0|     ha9c7c90_105        12.4 MB  conda-forge\n",
            "    libstdcxx-ng-13.2.0        |       h7e041cc_5         3.7 MB  conda-forge\n",
            "    libtasn1-4.19.0            |       h166bdaf_0         114 KB  conda-forge\n",
            "    libunistring-0.9.10        |       h7f98852_0         1.4 MB  conda-forge\n",
            "    libuuid-2.38.1             |       h0b41bf4_0          33 KB  conda-forge\n",
            "    libuv-1.46.0               |       hd590300_0         872 KB  conda-forge\n",
            "    libva-2.20.0               |       hd590300_0         184 KB  conda-forge\n",
            "    libvpx-1.13.1              |       h59595ed_0         982 KB  conda-forge\n",
            "    libxcb-1.15                |       h0b41bf4_0         375 KB  conda-forge\n",
            "    libxml2-2.12.5             |       h232c23b_0         688 KB  conda-forge\n",
            "    libzlib-1.2.13             |       hd590300_5          60 KB  conda-forge\n",
            "    lzo-2.10                   |    h516909a_1000         314 KB  conda-forge\n",
            "    nettle-3.9.1               |       h7ab15ed_0         988 KB  conda-forge\n",
            "    ocl-icd-2.3.1              |       h7f98852_0         119 KB  conda-forge\n",
            "    ocl-icd-system-1.0.0       |                1           4 KB  conda-forge\n",
            "    openh264-2.4.1             |       h59595ed_0         718 KB  conda-forge\n",
            "    openssl-3.2.1              |       hd590300_0         2.7 MB  conda-forge\n",
            "    p11-kit-0.24.1             |       hc5aa10d_0         4.5 MB  conda-forge\n",
            "    pixman-0.43.2              |       h59595ed_0         378 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h36c2ea0_1001           5 KB  conda-forge\n",
            "    pugixml-1.14               |       h59595ed_0         112 KB  conda-forge\n",
            "    rhash-1.4.4                |       hd590300_0         181 KB  conda-forge\n",
            "    snappy-1.1.10              |       h9fff704_0          38 KB  conda-forge\n",
            "    svt-av1-1.8.0              |       h59595ed_0         2.5 MB  conda-forge\n",
            "    sysroot_linux-64-2.12      |      he073ed8_16        14.6 MB  conda-forge\n",
            "    tbb-2021.11.0              |       h00ab1b0_1         191 KB  conda-forge\n",
            "    x264-1!164.3095            |       h166bdaf_2         877 KB  conda-forge\n",
            "    x265-3.5                   |       h924138e_3         3.2 MB  conda-forge\n",
            "    xorg-fixesproto-5.0        |    h7f98852_1002           9 KB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    h7f98852_1002          27 KB  conda-forge\n",
            "    xorg-libice-1.1.1          |       hd590300_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.4           |       h7391055_0          27 KB  conda-forge\n",
            "    xorg-libx11-1.8.7          |       h8ee46fc_0         809 KB  conda-forge\n",
            "    xorg-libxau-1.0.11         |       hd590300_0          14 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h7f98852_0          19 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h0b41bf4_2          49 KB  conda-forge\n",
            "    xorg-libxfixes-5.0.3       |    h7f98852_1004          18 KB  conda-forge\n",
            "    xorg-libxrender-0.9.11     |       hd590300_0          37 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    h7f98852_1002           9 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    h0b41bf4_1003          30 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    h7f98852_1007          73 KB  conda-forge\n",
            "    zlib-1.2.13                |       hd590300_5          91 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       227.3 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  aom                conda-forge/linux-64::aom-3.8.1-h59595ed_0 \n",
            "  binutils_impl_lin~ conda-forge/linux-64::binutils_impl_linux-64-2.40-hf600244_0 \n",
            "  cairo              conda-forge/linux-64::cairo-1.18.0-h3faef2a_0 \n",
            "  cmake              conda-forge/linux-64::cmake-3.28.2-hcfe8598_0 \n",
            "  dav1d              conda-forge/linux-64::dav1d-1.2.1-hd590300_0 \n",
            "  expat              conda-forge/linux-64::expat-2.5.0-hcb278e6_1 \n",
            "  ffmpeg             conda-forge/linux-64::ffmpeg-6.1.1-gpl_h8007c5b_104 \n",
            "  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0 \n",
            "  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0 \n",
            "  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0 \n",
            "  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-h77eed37_1 \n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.14.2-h14ed4e7_0 \n",
            "  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0 \n",
            "  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-0 \n",
            "  freetype           conda-forge/linux-64::freetype-2.12.1-h267a509_2 \n",
            "  fribidi            conda-forge/linux-64::fribidi-1.0.10-h36c2ea0_0 \n",
            "  gcc                conda-forge/linux-64::gcc-13.2.0-h574f8da_2 \n",
            "  gcc_impl_linux-64  conda-forge/linux-64::gcc_impl_linux-64-13.2.0-h338b0a0_5 \n",
            "  gettext            conda-forge/linux-64::gettext-0.21.1-h27087fc_0 \n",
            "  gmp                conda-forge/linux-64::gmp-6.3.0-h59595ed_0 \n",
            "  gnutls             conda-forge/linux-64::gnutls-3.7.9-hb077bed_0 \n",
            "  graphite2          conda-forge/linux-64::graphite2-1.3.13-h58526e2_1001 \n",
            "  gxx                conda-forge/linux-64::gxx-13.2.0-h574f8da_2 \n",
            "  gxx_impl_linux-64  conda-forge/linux-64::gxx_impl_linux-64-13.2.0-h338b0a0_5 \n",
            "  harfbuzz           conda-forge/linux-64::harfbuzz-8.3.0-h3d44ed6_0 \n",
            "  kernel-headers_li~ conda-forge/noarch::kernel-headers_linux-64-2.6.32-he073ed8_16 \n",
            "  lame               conda-forge/linux-64::lame-3.100-h166bdaf_1003 \n",
            "  libabseil          conda-forge/linux-64::libabseil-20230802.1-cxx17_h59595ed_0 \n",
            "  libass             conda-forge/linux-64::libass-0.17.1-h8fe9dca_1 \n",
            "  libdrm             conda-forge/linux-64::libdrm-2.4.114-h166bdaf_0 \n",
            "  libexpat           conda-forge/linux-64::libexpat-2.5.0-hcb278e6_1 \n",
            "  libgcc-devel_linu~ conda-forge/noarch::libgcc-devel_linux-64-13.2.0-ha9c7c90_105 \n",
            "  libglib            conda-forge/linux-64::libglib-2.78.3-h783c2da_0 \n",
            "  libhwloc           conda-forge/linux-64::libhwloc-2.9.3-default_h554bfaf_1009 \n",
            "  libiconv           conda-forge/linux-64::libiconv-1.17-hd590300_2 \n",
            "  libidn2            conda-forge/linux-64::libidn2-2.3.7-hd590300_0 \n",
            "  libopenvino        conda-forge/linux-64::libopenvino-2023.3.0-h2e90f83_0 \n",
            "  libopenvino-auto-~ conda-forge/linux-64::libopenvino-auto-batch-plugin-2023.3.0-hd5fc58b_0 \n",
            "  libopenvino-auto-~ conda-forge/linux-64::libopenvino-auto-plugin-2023.3.0-hd5fc58b_0 \n",
            "  libopenvino-heter~ conda-forge/linux-64::libopenvino-hetero-plugin-2023.3.0-h3ecfda7_0 \n",
            "  libopenvino-intel~ conda-forge/linux-64::libopenvino-intel-cpu-plugin-2023.3.0-h2e90f83_0 \n",
            "  libopenvino-intel~ conda-forge/linux-64::libopenvino-intel-gpu-plugin-2023.3.0-h2e90f83_0 \n",
            "  libopenvino-ir-fr~ conda-forge/linux-64::libopenvino-ir-frontend-2023.3.0-h3ecfda7_0 \n",
            "  libopenvino-onnx-~ conda-forge/linux-64::libopenvino-onnx-frontend-2023.3.0-hfbc7f12_0 \n",
            "  libopenvino-paddl~ conda-forge/linux-64::libopenvino-paddle-frontend-2023.3.0-hfbc7f12_0 \n",
            "  libopenvino-pytor~ conda-forge/linux-64::libopenvino-pytorch-frontend-2023.3.0-h59595ed_0 \n",
            "  libopenvino-tenso~ conda-forge/linux-64::libopenvino-tensorflow-frontend-2023.3.0-h0bff32c_0 \n",
            "  libopenvino-tenso~ conda-forge/linux-64::libopenvino-tensorflow-lite-frontend-2023.3.0-h59595ed_0 \n",
            "  libopus            conda-forge/linux-64::libopus-1.3.1-h7f98852_1 \n",
            "  libpciaccess       conda-forge/linux-64::libpciaccess-0.17-h166bdaf_0 \n",
            "  libpng             conda-forge/linux-64::libpng-1.6.42-h2797004_0 \n",
            "  libprotobuf        conda-forge/linux-64::libprotobuf-4.25.1-hf27288f_1 \n",
            "  libsanitizer       conda-forge/linux-64::libsanitizer-13.2.0-h7e041cc_5 \n",
            "  libstdcxx-devel_l~ conda-forge/noarch::libstdcxx-devel_linux-64-13.2.0-ha9c7c90_105 \n",
            "  libtasn1           conda-forge/linux-64::libtasn1-4.19.0-h166bdaf_0 \n",
            "  libunistring       conda-forge/linux-64::libunistring-0.9.10-h7f98852_0 \n",
            "  libuuid            conda-forge/linux-64::libuuid-2.38.1-h0b41bf4_0 \n",
            "  libuv              conda-forge/linux-64::libuv-1.46.0-hd590300_0 \n",
            "  libva              conda-forge/linux-64::libva-2.20.0-hd590300_0 \n",
            "  libvpx             conda-forge/linux-64::libvpx-1.13.1-h59595ed_0 \n",
            "  libxcb             conda-forge/linux-64::libxcb-1.15-h0b41bf4_0 \n",
            "  libzlib            conda-forge/linux-64::libzlib-1.2.13-hd590300_5 \n",
            "  lzo                conda-forge/linux-64::lzo-2.10-h516909a_1000 \n",
            "  nettle             conda-forge/linux-64::nettle-3.9.1-h7ab15ed_0 \n",
            "  ocl-icd            conda-forge/linux-64::ocl-icd-2.3.1-h7f98852_0 \n",
            "  ocl-icd-system     conda-forge/linux-64::ocl-icd-system-1.0.0-1 \n",
            "  openh264           conda-forge/linux-64::openh264-2.4.1-h59595ed_0 \n",
            "  p11-kit            conda-forge/linux-64::p11-kit-0.24.1-hc5aa10d_0 \n",
            "  pixman             conda-forge/linux-64::pixman-0.43.2-h59595ed_0 \n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h36c2ea0_1001 \n",
            "  pugixml            conda-forge/linux-64::pugixml-1.14-h59595ed_0 \n",
            "  rhash              conda-forge/linux-64::rhash-1.4.4-hd590300_0 \n",
            "  snappy             conda-forge/linux-64::snappy-1.1.10-h9fff704_0 \n",
            "  svt-av1            conda-forge/linux-64::svt-av1-1.8.0-h59595ed_0 \n",
            "  sysroot_linux-64   conda-forge/noarch::sysroot_linux-64-2.12-he073ed8_16 \n",
            "  tbb                conda-forge/linux-64::tbb-2021.11.0-h00ab1b0_1 \n",
            "  x264               conda-forge/linux-64::x264-1!164.3095-h166bdaf_2 \n",
            "  x265               conda-forge/linux-64::x265-3.5-h924138e_3 \n",
            "  xorg-fixesproto    conda-forge/linux-64::xorg-fixesproto-5.0-h7f98852_1002 \n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h7f98852_1002 \n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.1.1-hd590300_0 \n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.4-h7391055_0 \n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.8.7-h8ee46fc_0 \n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.11-hd590300_0 \n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h7f98852_0 \n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h0b41bf4_2 \n",
            "  xorg-libxfixes     conda-forge/linux-64::xorg-libxfixes-5.0.3-h7f98852_1004 \n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.11-hd590300_0 \n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h7f98852_1002 \n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h0b41bf4_1003 \n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h7f98852_1007 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2023.12.12~ --> conda-forge::ca-certificates-2024.2.2-hbcca054_0 \n",
            "  certifi            pkgs/main/linux-64::certifi-2023.11.1~ --> conda-forge/noarch::certifi-2024.2.2-pyhd8ed1ab_0 \n",
            "  conda                              23.11.0-py39h06a4308_0 --> 24.1.0-py39h06a4308_0 \n",
            "  icu                        pkgs/main::icu-73.1-h6a678d5_0 --> conda-forge::icu-73.2-h59595ed_0 \n",
            "  ld_impl_linux-64   pkgs/main::ld_impl_linux-64-2.38-h118~ --> conda-forge::ld_impl_linux-64-2.40-h41732ed_0 \n",
            "  libcurl                                  8.4.0-h251f7ec_1 --> 8.5.0-h251f7ec_0 \n",
            "  libgcc-ng          pkgs/main::libgcc-ng-11.2.0-h1234567_1 --> conda-forge::libgcc-ng-13.2.0-h807b86a_5 \n",
            "  libgomp              pkgs/main::libgomp-11.2.0-h1234567_1 --> conda-forge::libgomp-13.2.0-h807b86a_5 \n",
            "  libstdcxx-ng       pkgs/main::libstdcxx-ng-11.2.0-h12345~ --> conda-forge::libstdcxx-ng-13.2.0-h7e041cc_5 \n",
            "  libxml2              pkgs/main::libxml2-2.10.4-hf1b16e4_1 --> conda-forge::libxml2-2.12.5-h232c23b_0 \n",
            "  openssl              pkgs/main::openssl-3.0.12-h7f8727e_0 --> conda-forge::openssl-3.2.1-hd590300_0 \n",
            "  zlib                    pkgs/main::zlib-1.2.13-h5eee18b_0 --> conda-forge::zlib-1.2.13-hd590300_5 \n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  _libgcc_mutex           pkgs/main::_libgcc_mutex-0.1-main --> conda-forge::_libgcc_mutex-0.1-conda_forge \n",
            "  _openmp_mutex          pkgs/main::_openmp_mutex-5.1-1_gnu --> conda-forge::_openmp_mutex-4.5-2_gnu \n",
            "  libarchive         pkgs/main::libarchive-3.6.2-h6ac8c49_2 --> conda-forge::libarchive-3.6.2-h039dbb9_1 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Collecting numpy (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.26.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from -r requirements.txt (line 2))\n",
            "  Downloading scipy-1.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard (from -r requirements.txt (line 3))\n",
            "  Downloading tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting librosa==0.9.2 (from -r requirements.txt (line 4))\n",
            "  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numba==0.56.4 (from -r requirements.txt (line 5))\n",
            "  Downloading numba-0.56.4-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning (from -r requirements.txt (line 6))\n",
            "  Downloading pytorch_lightning-2.1.4-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting gradio==3.38.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-3.38.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting ffmpeg-python (from -r requirements.txt (line 8))\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting onnxruntime (from -r requirements.txt (line 9))\n",
            "  Downloading onnxruntime-1.17.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (4.65.0)\n",
            "Collecting funasr>=1.0.0 (from -r requirements.txt (line 11))\n",
            "  Downloading funasr-1.0.5-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting cn2an (from -r requirements.txt (line 12))\n",
            "  Downloading cn2an-0.5.22-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pypinyin (from -r requirements.txt (line 13))\n",
            "  Downloading pypinyin-0.50.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pyopenjtalk (from -r requirements.txt (line 14))\n",
            "  Downloading pyopenjtalk-0.3.3.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting g2p_en (from -r requirements.txt (line 15))\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio (from -r requirements.txt (line 16))\n",
            "  Downloading torchaudio-2.2.0-cp39-cp39-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting modelscope==1.10.0 (from -r requirements.txt (line 17))\n",
            "  Downloading modelscope-1.10.0-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting sentencepiece (from -r requirements.txt (line 18))\n",
            "  Downloading sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers (from -r requirements.txt (line 19))\n",
            "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet (from -r requirements.txt (line 20))\n",
            "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting PyYAML (from -r requirements.txt (line 21))\n",
            "  Downloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting psutil (from -r requirements.txt (line 22))\n",
            "  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting jieba_fast (from -r requirements.txt (line 23))\n",
            "  Downloading jieba_fast-0.53.tar.gz (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba (from -r requirements.txt (line 24))\n",
            "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting LangSegment (from -r requirements.txt (line 25))\n",
            "  Downloading LangSegment-0.1.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting audioread>=2.1.9 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting scikit-learn>=0.19.1 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading scikit_learn-1.4.0-1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting joblib>=0.14 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting decorator>=4.0.10 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting resampy>=0.2.2 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting soundfile>=0.10.2 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pooch>=1.0 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading pooch-1.8.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (23.1)\n",
            "Collecting llvmlite<0.40,>=0.39.0dev0 (from numba==0.56.4->-r requirements.txt (line 5))\n",
            "  Downloading llvmlite-0.39.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from numba==0.56.4->-r requirements.txt (line 5)) (68.2.2)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting aiohttp~=3.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting altair<6.0,>=4.2.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting fastapi (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading fastapi-0.109.2-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting ffmpy (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.2.10 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.8.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting huggingface-hub>=0.14.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting jinja2<4.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting markdown-it-py>=2.0.0 (from markdown-it-py[linkify]>=2.0.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting markupsafe~=2.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting matplotlib~=3.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading matplotlib-3.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson~=3.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading orjson-3.9.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas<3.0,>=1.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pandas-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting pillow<11.0,>=8.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pillow-10.2.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pydantic-2.6.0-py3-none-any.whl.metadata (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading python_multipart-0.0.7-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.9/site-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (2.31.0)\n",
            "Collecting semantic-version~=2.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting typing-extensions~=4.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading uvicorn-0.27.0.post1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting attrs (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting datasets>=2.14.5 (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting einops (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting filelock>=3.3.0 (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting gast>=0.2.2 (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
            "Collecting oss2 (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading oss2-2.18.4.tar.gz (278 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyarrow!=9.0.0,>=6.0.0 (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading pyarrow-15.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting python-dateutil>=2.1 (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simplejson>=3.3.0 (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading simplejson-3.19.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
            "Collecting sortedcontainers>=1.5.9 (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 17)) (1.26.18)\n",
            "Collecting yapf (from modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading yapf-0.40.2-py3-none-any.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading grpcio-1.60.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting google-auth<3,>=1.6.3 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading google_auth-2.27.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading Markdown-3.5.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting protobuf<4.24,>=3.19.6 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
            "Collecting six>1.9 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting torch>=1.12.0 (from pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading torch-2.2.0-cp39-cp39-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting fsspec>=2022.5.0 (from fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting future (from ffmpeg-python->-r requirements.txt (line 8))\n",
            "  Downloading future-0.18.3.tar.gz (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting coloredlogs (from onnxruntime->-r requirements.txt (line 9))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flatbuffers (from onnxruntime->-r requirements.txt (line 9))\n",
            "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Collecting sympy (from onnxruntime->-r requirements.txt (line 9))\n",
            "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jamo (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Collecting kaldiio>=2.17.0 (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading kaldiio-2.18.0-py3-none-any.whl (28 kB)\n",
            "Collecting torch-complex (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading torch_complex-0.4.3-py3-none-any.whl (9.1 kB)\n",
            "Collecting pytorch-wpe (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading pytorch_wpe-0.0.1-py3-none-any.whl (8.1 kB)\n",
            "Collecting editdistance>=0.5.2 (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading editdistance-0.6.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.4/282.4 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hdbscan (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jaconv (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading jaconv-0.3.4.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hydra-core>=1.3.2 (from funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting proces>=0.1.3 (from cn2an->-r requirements.txt (line 12))\n",
            "  Downloading proces-0.1.7-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting nltk>=3.2.4 (from g2p_en->-r requirements.txt (line 15))\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting inflect>=0.3.1 (from g2p_en->-r requirements.txt (line 15))\n",
            "  Downloading inflect-7.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting distance>=0.1.3 (from g2p_en->-r requirements.txt (line 15))\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting networkx (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2.0 (from torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading triton-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.12.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 19))\n",
            "  Downloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.19,>=0.14 (from transformers->-r requirements.txt (line 19))\n",
            "  Downloading tokenizers-0.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers->-r requirements.txt (line 19))\n",
            "  Downloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting py3langid>=0.2.2 (from LangSegment->-r requirements.txt (line 25))\n",
            "  Downloading py3langid-0.2.2-py3-none-any.whl (750 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting jsonschema>=3.0 (from altair<6.0,>=4.2.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting toolz (from altair<6.0,>=4.2.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting xxhash (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2022.5.0 (from fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.3.2->funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.3.2->funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading importlib_metadata-7.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.0.0->markdown-it-py[linkify]>=2.0.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading linkify_it_py-2.0.3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading contourpy-1.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading fonttools-4.47.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (157 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.6/157.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting importlib-resources>=3.2.0 (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is still looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click (from nltk>=3.2.4->g2p_en->-r requirements.txt (line 15))\n",
            "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pytz>=2020.1 (from pandas<3.0,>=1.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas<3.0,>=1.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading tzdata-2023.4-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.9/site-packages (from pooch>=1.0->librosa==0.9.2->-r requirements.txt (line 4)) (3.10.0)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pydantic-core==2.16.1 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pydantic_core-2.16.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests~=2.0->gradio==3.38.0->-r requirements.txt (line 7)) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests~=2.0->gradio==3.38.0->-r requirements.txt (line 7)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests~=2.0->gradio==3.38.0->-r requirements.txt (line 7)) (2024.2.2)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.19.1->librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/site-packages (from soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 4)) (1.16.0)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->-r requirements.txt (line 9))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.37.0,>=0.36.3 (from fastapi->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting cython<3,>=0.27 (from hdbscan->funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Using cached Cython-0.29.37-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.1 kB)\n",
            "Collecting anyio (from httpx->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting httpcore==1.* (from httpx->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting sniffio (from httpx->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting crcmod>=1.7 (from oss2->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome>=3.4.7 (from oss2->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting aliyun-python-sdk-kms>=2.4.1 (from oss2->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading aliyun_python_sdk_kms-2.16.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aliyun-python-sdk-core>=2.13.12 (from oss2->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading aliyun-python-sdk-core-2.14.0.tar.gz (443 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.0/443.0 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mpmath>=0.19 (from sympy->onnxruntime->-r requirements.txt (line 9))\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynndescent>=0.5 (from umap-learn->funasr>=1.0.0->-r requirements.txt (line 11))\n",
            "  Downloading pynndescent-0.5.11-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting tomli>=2.0.1 (from yapf->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cryptography>=2.6.0 in /usr/local/lib/python3.9/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->modelscope==1.10.0->-r requirements.txt (line 17)) (41.0.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 4)) (2.21)\n",
            "Collecting zipp>=0.5 (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading referencing-0.33.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading rpds_py-0.17.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting exceptiongroup>=1.0.2 (from anyio->httpx->gradio==3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 17))\n",
            "  Downloading multiprocess-0.70.15-py39-none-any.whl.metadata (7.2 kB)\n",
            "Downloading gradio-3.38.0-py3-none-any.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading modelscope-1.10.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.5/38.5 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.1.4-py3-none-any.whl (778 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.1/778.1 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.17.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funasr-1.0.5-py3-none-any.whl (549 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.6/549.6 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cn2an-0.5.22-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypinyin-0.50.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.2.0-cp39-cp39-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.0-cp39-cp39-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m738.9/738.9 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading LangSegment-0.1.3-py3-none-any.whl (6.9 kB)\n",
            "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.9/996.9 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
            "Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth-2.27.0-py2.py3-none-any.whl (186 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.8/186.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading gradio_client-0.8.1-py3-none-any.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.60.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inflect-7.0.0-py3-none-any.whl (34 kB)\n",
            "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading matplotlib-3.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.9.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.6/138.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.2.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pooch-1.8.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading proces-0.1.7-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-15.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.6.0-py3-none-any.whl (394 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.16.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.4/773.4 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.0-1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simplejson-3.19.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Downloading uvicorn-0.27.0.post1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.109.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
            "Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.7-py3-none-any.whl (22 kB)\n",
            "Downloading yapf-0.40.2-py3-none-any.whl (254 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aliyun_python_sdk_kms-2.16.2-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
            "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.0/311.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached Cython-0.29.37-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fonttools-4.47.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\n",
            "Downloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
            "Downloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linkify_it_py-2.0.3-py3-none-any.whl (19 kB)\n",
            "Downloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.11-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyio-4.2.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Downloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.3/304.3 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.8/193.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
            "Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading referencing-0.33.0-py3-none-any.whl (26 kB)\n",
            "Downloading rpds_py-0.17.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyopenjtalk, jieba_fast, jieba, distance, antlr4-python3-runtime, ffmpy, future, hdbscan, jaconv, oss2, umap-learn, aliyun-python-sdk-core, crcmod\n",
            "  Building wheel for pyopenjtalk (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyopenjtalk: filename=pyopenjtalk-0.3.3-cp39-cp39-linux_x86_64.whl size=1195586 sha256=edee248e1e2db46e6f13c8ded13bdd2a05e3373d0d0df239f823807ffeae0c5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/1e/46/f15c401c93d6fdf1dba5c9268033c057d033f6243e8f51e806\n",
            "  Building wheel for jieba_fast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba_fast: filename=jieba_fast-0.53-cp39-cp39-linux_x86_64.whl size=7629272 sha256=898b209f1dff83a04bb04ec9db901ce3b34d7da412603329a2e4cbd9a60c3020\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/04/c8/5c563e7f58588aadfa5af1353a086ef467eb1dadd2fcfac622\n",
            "  Building wheel for jieba (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=18070a02f2a535d84134275601332d8d6594636fda53e45204e8b5ec6262c048\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=11a8f17070aa7b54a7458baae665702c6d2106c5307e9de249f0b03c788de338\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/b3/aa/04241cced6d1722b132273b1d6aafba317887ec004f48b853a\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=e744a4515c02c20b37a41775a3d335d9e8a70219137d0aae186fef6485a047eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/cf/80/f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=e577d597add931cfb949e5fd0cad3a3dfc73ec09035cc82b4009cfda599e37b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/f1/8d/367922b023b526b7c2ced5db30932def7b18cf39d7ac6e8572\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492024 sha256=6fcbabf518511b159899e3dc7c560eefca7c93f62c699eecf1fe53c75635dd48\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/5d/6a/2e53874f7ec4e2bede522385439531fafec8fafe005b5c3d1b\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp39-cp39-linux_x86_64.whl size=737965 sha256=880c623fd1c85d175ba14eb77b3e19c869714c51b3f18d1728baa8316fdcbd4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/5e/ed/5989da4cc423a222a47cbb4fde5d6c0eff4590d922e45f233c\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaconv: filename=jaconv-0.3.4-py3-none-any.whl size=16416 sha256=370224e2dd976b7a877e616bf036a1a0178120345b6d15697ad45b21c1ec73e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/de/f1/55f605920db3666d30215331bc85f24686dde9b95b473ae41b\n",
            "  Building wheel for oss2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oss2: filename=oss2-2.18.4-py3-none-any.whl size=115939 sha256=1dbb3425deb6770462c9e4307c588a22ac6e3348d164ce1626f0f23343fa7b7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/65/8e/0417b076271c1d16cd41db00820b121cdc3fab91e2fbeb2aae\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.5-py3-none-any.whl size=86832 sha256=cdd8a424bbe49836d8afc2523ccc190e36f91527924c549ed625401120e28dc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/01/17/26951217a11fb724b7027c6dd5b620b6d368104f7e6d4171fc\n",
            "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.14.0-py3-none-any.whl size=535290 sha256=44050033a179254e35330c438fea4a4595bb66c5b51ca102c8a9050cdb1c33b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/bd/a9/ef224c78c3aa426c4867927f46ebb6fd1fd705c6c4bafde111\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp39-cp39-linux_x86_64.whl size=23291 sha256=39c83740db72803a3a6ee669cf9ee8b61abfdcdaffbbaac3383156cbf2e78db1\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/6c/a6/ffdd136310039bf226f2707a9a8e6857be7d70a3fc061f6b36\n",
            "Successfully built pyopenjtalk jieba_fast jieba distance antlr4-python3-runtime ffmpy future hdbscan jaconv oss2 umap-learn aliyun-python-sdk-core crcmod\n",
            "Installing collected packages: sortedcontainers, sentencepiece, pytz, pydub, mpmath, jieba_fast, jieba, jamo, jaconv, flatbuffers, ffmpy, distance, crcmod, antlr4-python3-runtime, addict, zipp, xxhash, websockets, uc-micro-py, tzdata, typing-extensions, toolz, tomli, threadpoolctl, tensorboard-data-server, sympy, sniffio, six, simplejson, semantic-version, safetensors, rpds-py, regex, PyYAML, python-multipart, pypinyin, pyparsing, pycryptodome, pyasn1, pyarrow-hotfix, psutil, protobuf, proces, pillow, orjson, oauthlib, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, mdurl, markupsafe, llvmlite, kiwisolver, joblib, jmespath, humanfriendly, h11, grpcio, gast, future, fsspec, frozenlist, fonttools, filelock, exceptiongroup, einops, editdistance, dill, decorator, cython, cycler, click, chardet, cachetools, audioread, attrs, async-timeout, annotated-types, aiofiles, absl-py, yarl, werkzeug, uvicorn, triton, torch-complex, soundfile, scipy, rsa, requests-oauthlib, referencing, pytorch-wpe, python-dateutil, pyopenjtalk, pydantic-core, pyasn1-modules, pyarrow, py3langid, pooch, omegaconf, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, nltk, multiprocess, markdown-it-py, linkify-it-py, lightning-utilities, kaldiio, jinja2, importlib-resources, importlib-metadata, huggingface-hub, httpcore, ffmpeg-python, contourpy, coloredlogs, cn2an, anyio, aiosignal, yapf, tokenizers, starlette, scikit-learn, resampy, pydantic, pandas, onnxruntime, nvidia-cusolver-cu12, mdit-py-plugins, matplotlib, markdown, LangSegment, jsonschema-specifications, hydra-core, httpx, google-auth, aliyun-python-sdk-core, aiohttp, transformers, torch, pynndescent, librosa, jsonschema, inflect, hdbscan, gradio-client, google-auth-oauthlib, fastapi, aliyun-python-sdk-kms, umap-learn, torchmetrics, torchaudio, tensorboard, oss2, g2p_en, datasets, altair, pytorch-lightning, modelscope, gradio, funasr\n",
            "Successfully installed LangSegment-0.1.3 PyYAML-6.0.1 absl-py-2.1.0 addict-2.4.0 aiofiles-23.2.1 aiohttp-3.9.3 aiosignal-1.3.1 aliyun-python-sdk-core-2.14.0 aliyun-python-sdk-kms-2.16.2 altair-5.2.0 annotated-types-0.6.0 antlr4-python3-runtime-4.9.3 anyio-4.2.0 async-timeout-4.0.3 attrs-23.2.0 audioread-3.0.1 cachetools-5.3.2 chardet-5.2.0 click-8.1.7 cn2an-0.5.22 coloredlogs-15.0.1 contourpy-1.2.0 crcmod-1.7 cycler-0.12.1 cython-0.29.37 datasets-2.16.1 decorator-5.1.1 dill-0.3.7 distance-0.1.3 editdistance-0.6.2 einops-0.7.0 exceptiongroup-1.2.0 fastapi-0.109.2 ffmpeg-python-0.2.0 ffmpy-0.3.1 filelock-3.13.1 flatbuffers-23.5.26 fonttools-4.47.2 frozenlist-1.4.1 fsspec-2023.10.0 funasr-1.0.5 future-0.18.3 g2p_en-2.1.0 gast-0.5.4 google-auth-2.27.0 google-auth-oauthlib-1.2.0 gradio-3.38.0 gradio-client-0.8.1 grpcio-1.60.1 h11-0.14.0 hdbscan-0.8.33 httpcore-1.0.2 httpx-0.26.0 huggingface-hub-0.20.3 humanfriendly-10.0 hydra-core-1.3.2 importlib-metadata-7.0.1 importlib-resources-6.1.1 inflect-7.0.0 jaconv-0.3.4 jamo-0.4.1 jieba-0.42.1 jieba_fast-0.53 jinja2-3.1.3 jmespath-0.10.0 joblib-1.3.2 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 kaldiio-2.18.0 kiwisolver-1.4.5 librosa-0.9.2 lightning-utilities-0.10.1 linkify-it-py-2.0.3 llvmlite-0.39.1 markdown-3.5.2 markdown-it-py-2.2.0 markupsafe-2.1.5 matplotlib-3.8.2 mdit-py-plugins-0.3.3 mdurl-0.1.2 modelscope-1.10.0 mpmath-1.3.0 multidict-6.0.5 multiprocess-0.70.15 networkx-3.2.1 nltk-3.8.1 numba-0.56.4 numpy-1.23.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 oauthlib-3.2.2 omegaconf-2.3.0 onnxruntime-1.17.0 orjson-3.9.13 oss2-2.18.4 pandas-2.2.0 pillow-10.2.0 pooch-1.8.0 proces-0.1.7 protobuf-4.23.4 psutil-5.9.8 py3langid-0.2.2 pyarrow-15.0.0 pyarrow-hotfix-0.6 pyasn1-0.5.1 pyasn1-modules-0.3.0 pycryptodome-3.20.0 pydantic-2.6.0 pydantic-core-2.16.1 pydub-0.25.1 pynndescent-0.5.11 pyopenjtalk-0.3.3 pyparsing-3.1.1 pypinyin-0.50.0 python-dateutil-2.8.2 python-multipart-0.0.7 pytorch-lightning-2.1.4 pytorch-wpe-0.0.1 pytz-2024.1 referencing-0.33.0 regex-2023.12.25 requests-oauthlib-1.3.1 resampy-0.4.2 rpds-py-0.17.1 rsa-4.9 safetensors-0.4.2 scikit-learn-1.4.0 scipy-1.12.0 semantic-version-2.10.0 sentencepiece-0.1.99 simplejson-3.19.2 six-1.16.0 sniffio-1.3.0 sortedcontainers-2.4.0 soundfile-0.12.1 starlette-0.36.3 sympy-1.12 tensorboard-2.15.1 tensorboard-data-server-0.7.2 threadpoolctl-3.2.0 tokenizers-0.15.1 tomli-2.0.1 toolz-0.12.1 torch-2.2.0 torch-complex-0.4.3 torchaudio-2.2.0 torchmetrics-1.3.0.post0 transformers-4.37.2 triton-2.2.0 typing-extensions-4.9.0 tzdata-2023.4 uc-micro-py-1.0.2 umap-learn-0.5.5 uvicorn-0.27.0.post1 websockets-11.0.3 werkzeug-3.0.1 xxhash-3.4.1 yapf-0.40.2 yarl-1.9.4 zipp-3.17.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download pretrained models 下载预训练模型\n",
        "!mkdir -p /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "!mkdir -p /content/GPT-SoVITS/tools/damo_asr/models\n",
        "!mkdir -p /content/GPT-SoVITS/tools/uvr5\n",
        "%cd /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "!git clone https://huggingface.co/lj1995/GPT-SoVITS\n",
        "%cd /content/GPT-SoVITS/tools/damo_asr/models\n",
        "!git clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git\n",
        "# @title UVR5 pretrains 安装uvr5模型\n",
        "%cd /content/GPT-SoVITS/tools/uvr5\n",
        "!git clone https://huggingface.co/Delik/uvr5_weights\n",
        "!git config core.sparseCheckout true\n",
        "!mv /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/* /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/"
      ],
      "metadata": {
        "id": "0NgxXg5sjv7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dbab5ab-985a-456f-9c98-0793baf6e905"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "/content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "fatal: destination path 'GPT-SoVITS' already exists and is not an empty directory.\n",
            "/content/GPT-SoVITS/tools/damo_asr/models\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Cloning into 'speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch'...\n",
            "remote: Enumerating objects: 465, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 465 (delta 38), reused 57 (delta 30), pack-reused 398\u001b[K\n",
            "Receiving objects: 100% (465/465), 1.12 GiB | 14.63 MiB/s, done.\n",
            "Resolving deltas: 100% (268/268), done.\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Cloning into 'speech_fsmn_vad_zh-cn-16k-common-pytorch'...\n",
            "remote: Enumerating objects: 180, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 180 (delta 16), reused 23 (delta 10), pack-reused 145\u001b[K\n",
            "Receiving objects: 100% (180/180), 4.66 MiB | 10.17 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Cloning into 'punc_ct-transformer_zh-cn-common-vocab272727-pytorch'...\n",
            "remote: Enumerating objects: 167, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 167 (delta 23), reused 30 (delta 14), pack-reused 120\u001b[K\n",
            "Receiving objects: 100% (167/167), 257.56 MiB | 31.91 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n",
            "/content/GPT-SoVITS/tools/uvr5\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Cloning into 'uvr5_weights'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 15 (delta 1), reused 0 (delta 0), pack-reused 3\u001b[K\n",
            "Unpacking objects: 100% (15/15), 3.25 KiB | 666.00 KiB/s, done.\n",
            "Filtering content: 100% (9/9), 594.44 MiB | 81.13 MiB/s, done.\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title launch WebUI 启动WebUI\n",
        "!/usr/local/bin/pip install ipykernel\n",
        "!sed -i '9s/False/True/' /content/GPT-SoVITS/config.py\n",
        "%cd /content/GPT-SoVITS/\n",
        "import requests\n",
        "%cd /content/GPT-SoVITS\n",
        "# 获取密钥\n",
        "response = requests.get('https://api64.ipify.org?format=json')\n",
        "external_ip = response.json()['ip']\n",
        "\n",
        "print(\"Your external IP address is:\", external_ip)\n",
        "# webui填入密钥\n",
        "!npx localtunnel --port 9874 & python  webui.py\n",
        "\n"
      ],
      "metadata": {
        "id": "4oRGUzkrk8C7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9daf0bf-fe58-4206-e756-0918ff1039eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.9/site-packages (6.29.0)\n",
            "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.9/site-packages (from ipykernel) (0.2.1)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.9/site-packages (from ipykernel) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.9/site-packages (from ipykernel) (8.18.1)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.9/site-packages (from ipykernel) (8.6.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.9/site-packages (from ipykernel) (5.7.1)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.9/site-packages (from ipykernel) (0.1.6)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.9/site-packages (from ipykernel) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from ipykernel) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/site-packages (from ipykernel) (5.9.8)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.9/site-packages (from ipykernel) (25.1.2)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.9/site-packages (from ipykernel) (6.4)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.9/site-packages (from ipykernel) (5.14.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (2.17.2)\n",
            "Requirement already satisfied: stack-data in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (1.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (7.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (3.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel) (3.17.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in /usr/local/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "/content/GPT-SoVITS\n",
            "/content/GPT-SoVITS\n",
            "Your external IP address is: 34.31.100.252\n",
            "/bin/bash: /usr/local/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.916s\n",
            "your url is: https://warm-areas-show.loca.lt\n",
            "Running on local URL:  http://0.0.0.0:9874\n",
            "Running on public URL: https://981881b4a749402bbc.gradio.live\n",
            "\"/usr/local/bin/python\" tools/uvr5/webui.py \"cuda\" True 9873 True\n",
            "Running on local URL:  http://0.0.0.0:9873\n",
            "Running on public URL: https://c17c7caf3ef368c2bf.gradio.live\n",
            "ffmpeg version 6.1.1 Copyright (c) 2000-2023 the FFmpeg developers\n",
            "  built with gcc 12.3.0 (conda-forge gcc 12.3.0-5)\n",
            "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1706918361713/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1706918361713/_build_env/bin/x86_64-conda-linux-gnu-cc --cxx=/home/conda/feedstock_root/build_artifacts/ffmpeg_1706918361713/_build_env/bin/x86_64-conda-linux-gnu-c++ --nm=/home/conda/feedstock_root/build_artifacts/ffmpeg_1706918361713/_build_env/bin/x86_64-conda-linux-gnu-nm --ar=/home/conda/feedstock_root/build_artifacts/ffmpeg_1706918361713/_build_env/bin/x86_64-conda-linux-gnu-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libharfbuzz --enable-libfontconfig --enable-libopenh264 --enable-libdav1d --enable-gnutls --enable-libmp3lame --enable-libvpx --enable-libass --enable-pthreads --enable-vaapi --enable-libopenvino --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1706918361713/_build_env/bin/pkg-config\n",
            "  libavutil      58. 29.100 / 58. 29.100\n",
            "  libavcodec     60. 31.102 / 60. 31.102\n",
            "  libavformat    60. 16.100 / 60. 16.100\n",
            "  libavdevice    60.  3.100 / 60.  3.100\n",
            "  libavfilter     9. 12.100 /  9. 12.100\n",
            "  libswscale      7.  5.100 /  7.  5.100\n",
            "  libswresample   4. 12.100 /  4. 12.100\n",
            "  libpostproc    57.  3.100 / 57.  3.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/GPT-SoVITS/TEMP/gradio/dd6341b66dd9ed74b0799f5da41624b2e8ada95c/1707105518.mp3':\n",
            "  Metadata:\n",
            "    major_brand     : mp42\n",
            "    minor_version   : 0\n",
            "    compatible_brands: isommp42\n",
            "    creation_time   : 2024-02-05T03:58:56.000000Z\n",
            "    com.android.version: 14\n",
            "  Duration: 00:05:03.85, start: 0.000000, bitrate: 257 kb/s\n",
            "  Stream #0:0[0x1](eng): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 256 kb/s (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2024-02-05T03:58:56.000000Z\n",
            "      handler_name    : SoundHandle\n",
            "      vendor_id       : [0][0][0][0]\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, wav, to '/content/GPT-SoVITS/TEMP/1707105518.mp3.reformatted.wav':\n",
            "  Metadata:\n",
            "    major_brand     : mp42\n",
            "    minor_version   : 0\n",
            "    compatible_brands: isommp42\n",
            "    com.android.version: 14\n",
            "    ISFT            : Lavf60.16.100\n",
            "  Stream #0:0(eng): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2024-02-05T03:58:56.000000Z\n",
            "      handler_name    : SoundHandle\n",
            "      vendor_id       : [0][0][0][0]\n",
            "      encoder         : Lavc60.31.102 pcm_s16le\n",
            "\u001b[1;35m[out#0/wav @ 0x5a3ba4c9bec0] \u001b[0mvideo:0kB audio:52343kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000146%\n",
            "size=   52343kB time=00:05:03.85 bitrate=1411.2kbits/s speed= 252x    \n",
            "/content/GPT-SoVITS/tools/uvr5/vr.py:62: FutureWarning: Pass sr=44100, mono=False as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  ) = librosa.core.load(  # 理论上librosa读取可能对某些音频有bug，应该上ffmpeg读取，但是太麻烦了弃坑\n",
            "/content/GPT-SoVITS/tools/uvr5/lib/lib_v5/spec_utils.py:81: FutureWarning: Pass n_fft=960 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  spec_right = librosa.stft(wave_right, n_fft, hop_length=hop_length)\n",
            "/content/GPT-SoVITS/tools/uvr5/vr.py:72: FutureWarning: Pass orig_sr=44100, target_sr=14700 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  X_wave[d] = librosa.core.resample(\n",
            "/content/GPT-SoVITS/tools/uvr5/lib/lib_v5/spec_utils.py:81: FutureWarning: Pass n_fft=512 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  spec_right = librosa.stft(wave_right, n_fft, hop_length=hop_length)\n",
            "/content/GPT-SoVITS/tools/uvr5/vr.py:72: FutureWarning: Pass orig_sr=14700, target_sr=7350 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  X_wave[d] = librosa.core.resample(\n",
            "/content/GPT-SoVITS/tools/uvr5/lib/lib_v5/spec_utils.py:81: FutureWarning: Pass n_fft=320 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  spec_right = librosa.stft(wave_right, n_fft, hop_length=hop_length)\n",
            "/content/GPT-SoVITS/tools/uvr5/vr.py:72: FutureWarning: Pass orig_sr=7350, target_sr=7350 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  X_wave[d] = librosa.core.resample(\n",
            "/content/GPT-SoVITS/tools/uvr5/lib/lib_v5/spec_utils.py:81: FutureWarning: Pass n_fft=640 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  spec_right = librosa.stft(wave_right, n_fft, hop_length=hop_length)\n",
            "100% 110/110 [00:08<00:00, 13.31it/s]\n",
            "/content/GPT-SoVITS/tools/uvr5/lib/lib_v5/spec_utils.py:396: FutureWarning: Pass orig_sr=7350, target_sr=7350 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  wave = librosa.resample(\n",
            "/content/GPT-SoVITS/tools/uvr5/lib/lib_v5/spec_utils.py:422: FutureWarning: Pass orig_sr=7350, target_sr=14700 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  wave = librosa.core.resample(wave2, bp[\"sr\"], sr, res_type=\"scipy\")\n",
            "/content/GPT-SoVITS/tools/uvr5/lib/lib_v5/spec_utils.py:422: FutureWarning: Pass orig_sr=14700, target_sr=44100 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  wave = librosa.core.resample(wave2, bp[\"sr\"], sr, res_type=\"scipy\")\n",
            "clean_empty_cache\n",
            "\"/usr/local/bin/python\" tools/slice_audio.py \"/content/GPT-SoVITS/output/uvr5_opt/vocal_1707105518.mp3.reformatted.wav_10.wav\" \"output/slicer_opt\" -34 4000 300 10 500 0.9 0.25 0 1\n",
            "执行完毕，请检查输出文件\n",
            "\"/usr/local/bin/python\" tools/damo_asr/cmd-asr.py \"/content/GPT-SoVITS/output/slicer_opt\"\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "rtf_avg: 0.095: 100% 1/1 [00:00<00:00,  2.24it/s]\n",
            "time cost vad: 0.448\n",
            "rtf_avg: 0.331, time_speech:  4.670, time_escape: 1.546: 100% 1/1 [00:01<00:00,  1.57s/it]\n",
            "rtf_avg: 0.020: 100% 1/1 [00:00<00:00,  9.26it/s]\n",
            "time cost vad: 0.108\n",
            "rtf_avg: 0.028, time_speech:  5.250, time_escape: 0.147: 100% 1/1 [00:00<00:00,  5.75it/s]\n",
            "rtf_avg: 0.020: 100% 1/1 [00:00<00:00,  8.52it/s]\n",
            "time cost vad: 0.118\n",
            "rtf_avg: 0.036, time_speech:  5.790, time_escape: 0.207: 100% 1/1 [00:00<00:00,  4.34it/s]\n",
            "rtf_avg: 0.021: 100% 1/1 [00:00<00:00, 11.17it/s]\n",
            "time cost vad: 0.090\n",
            "rtf_avg: 0.036, time_speech:  4.290, time_escape: 0.155: 100% 1/1 [00:00<00:00,  5.59it/s]\n",
            "rtf_avg: 0.022: 100% 1/1 [00:00<00:00,  8.56it/s]\n",
            "time cost vad: 0.117\n",
            "rtf_avg: 0.031, time_speech:  5.180, time_escape: 0.163: 100% 1/1 [00:00<00:00,  5.37it/s]\n",
            "rtf_avg: 0.023: 100% 1/1 [00:00<00:00, 10.06it/s]\n",
            "time cost vad: 0.100\n",
            "rtf_avg: 0.038, time_speech:  4.320, time_escape: 0.163: 100% 1/1 [00:00<00:00,  5.20it/s]\n",
            "rtf_avg: 0.016: 100% 1/1 [00:00<00:00,  9.66it/s]\n",
            "time cost vad: 0.104\n",
            "rtf_avg: 0.018, time_speech:  6.270, time_escape: 0.116: 100% 1/1 [00:00<00:00,  7.56it/s]\n",
            "rtf_avg: 0.017: 100% 1/1 [00:00<00:00, 15.96it/s]\n",
            "time cost vad: 0.063\n",
            "rtf_avg: 0.036, time_speech:  3.580, time_escape: 0.129: 100% 1/1 [00:00<00:00,  6.98it/s]\n",
            "rtf_avg: 0.016: 100% 1/1 [00:00<00:00, 12.70it/s]\n",
            "time cost vad: 0.079\n",
            "rtf_avg: 0.023, time_speech:  4.770, time_escape: 0.111: 100% 1/1 [00:00<00:00,  7.87it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 11.92it/s]\n",
            "time cost vad: 0.084\n",
            "rtf_avg: 0.019, time_speech:  6.190, time_escape: 0.117: 100% 1/1 [00:00<00:00,  7.49it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 14.22it/s]\n",
            "time cost vad: 0.071\n",
            "rtf_avg: 0.020, time_speech:  5.360, time_escape: 0.105: 100% 1/1 [00:00<00:00,  8.31it/s]\n",
            "rtf_avg: 0.012: 100% 1/1 [00:00<00:00, 13.72it/s]\n",
            "time cost vad: 0.073\n",
            "rtf_avg: 0.018, time_speech:  5.920, time_escape: 0.105: 100% 1/1 [00:00<00:00,  8.32it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 18.32it/s]\n",
            "time cost vad: 0.055\n",
            "rtf_avg: 0.023, time_speech:  4.260, time_escape: 0.100: 100% 1/1 [00:00<00:00,  8.86it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 15.94it/s]\n",
            "time cost vad: 0.063\n",
            "rtf_avg: 0.023, time_speech:  4.620, time_escape: 0.107: 100% 1/1 [00:00<00:00,  8.25it/s]\n",
            "rtf_avg: 0.012: 100% 1/1 [00:00<00:00, 12.33it/s]\n",
            "time cost vad: 0.081\n",
            "rtf_avg: 0.019, time_speech:  6.810, time_escape: 0.128: 100% 1/1 [00:00<00:00,  6.90it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 15.29it/s]\n",
            "time cost vad: 0.066\n",
            "rtf_avg: 0.022, time_speech:  4.710, time_escape: 0.101: 100% 1/1 [00:00<00:00,  8.70it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 19.69it/s]\n",
            "time cost vad: 0.051\n",
            "rtf_avg: 0.028, time_speech:  3.690, time_escape: 0.102: 100% 1/1 [00:00<00:00,  8.63it/s]\n",
            "rtf_avg: 0.012: 100% 1/1 [00:00<00:00, 15.62it/s]\n",
            "time cost vad: 0.064\n",
            "rtf_avg: 0.019, time_speech:  5.250, time_escape: 0.102: 100% 1/1 [00:00<00:00,  8.60it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 21.90it/s]\n",
            "time cost vad: 0.046\n",
            "rtf_avg: 0.033, time_speech:  3.240, time_escape: 0.106: 100% 1/1 [00:00<00:00,  8.39it/s]\n",
            "rtf_avg: 0.017: 100% 1/1 [00:00<00:00, 14.52it/s]\n",
            "time cost vad: 0.069\n",
            "rtf_avg: 0.024, time_speech:  4.100, time_escape: 0.097: 100% 1/1 [00:00<00:00,  8.97it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 17.74it/s]\n",
            "time cost vad: 0.057\n",
            "rtf_avg: 0.026, time_speech:  4.120, time_escape: 0.106: 100% 1/1 [00:00<00:00,  8.34it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 13.76it/s]\n",
            "time cost vad: 0.073\n",
            "rtf_avg: 0.019, time_speech:  5.780, time_escape: 0.109: 100% 1/1 [00:00<00:00,  8.11it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 17.80it/s]\n",
            "time cost vad: 0.056\n",
            "rtf_avg: 0.025, time_speech:  4.170, time_escape: 0.106: 100% 1/1 [00:00<00:00,  8.36it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 13.24it/s]\n",
            "time cost vad: 0.076\n",
            "rtf_avg: 0.019, time_speech:  5.870, time_escape: 0.113: 100% 1/1 [00:00<00:00,  7.83it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 16.35it/s]\n",
            "time cost vad: 0.061\n",
            "rtf_avg: 0.026, time_speech:  4.400, time_escape: 0.114: 100% 1/1 [00:00<00:00,  7.86it/s]\n",
            "rtf_avg: 0.015: 100% 1/1 [00:00<00:00, 19.09it/s]\n",
            "time cost vad: 0.053\n",
            "rtf_avg: 0.027, time_speech:  3.590, time_escape: 0.098: 100% 1/1 [00:00<00:00,  9.01it/s]\n",
            "rtf_avg: 0.012: 100% 1/1 [00:00<00:00, 13.16it/s]\n",
            "time cost vad: 0.076\n",
            "rtf_avg: 0.019, time_speech:  6.220, time_escape: 0.120: 100% 1/1 [00:00<00:00,  7.44it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 18.60it/s]\n",
            "time cost vad: 0.054\n",
            "rtf_avg: 0.027, time_speech:  3.790, time_escape: 0.101: 100% 1/1 [00:00<00:00,  8.67it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 16.49it/s]\n",
            "time cost vad: 0.061\n",
            "rtf_avg: 0.021, time_speech:  4.830, time_escape: 0.101: 100% 1/1 [00:00<00:00,  8.78it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 16.78it/s]\n",
            "time cost vad: 0.060\n",
            "rtf_avg: 0.022, time_speech:  4.720, time_escape: 0.102: 100% 1/1 [00:00<00:00,  8.63it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 16.59it/s]\n",
            "time cost vad: 0.061\n",
            "rtf_avg: 0.023, time_speech:  4.340, time_escape: 0.100: 100% 1/1 [00:00<00:00,  8.27it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 13.38it/s]\n",
            "time cost vad: 0.075\n",
            "rtf_avg: 0.019, time_speech:  5.730, time_escape: 0.107: 100% 1/1 [00:00<00:00,  8.28it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 18.99it/s]\n",
            "time cost vad: 0.053\n",
            "rtf_avg: 0.026, time_speech:  3.980, time_escape: 0.105: 100% 1/1 [00:00<00:00,  8.53it/s]\n",
            "rtf_avg: 0.012: 100% 1/1 [00:00<00:00, 16.30it/s]\n",
            "time cost vad: 0.062\n",
            "rtf_avg: 0.020, time_speech:  4.930, time_escape: 0.099: 100% 1/1 [00:00<00:00,  8.91it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 15.24it/s]\n",
            "time cost vad: 0.066\n",
            "rtf_avg: 0.021, time_speech:  5.050, time_escape: 0.105: 100% 1/1 [00:00<00:00,  8.46it/s]\n",
            "rtf_avg: 0.012: 100% 1/1 [00:00<00:00, 17.40it/s]\n",
            "time cost vad: 0.058\n",
            "rtf_avg: 0.020, time_speech:  4.720, time_escape: 0.096: 100% 1/1 [00:00<00:00,  9.08it/s]\n",
            "rtf_avg: 0.016: 100% 1/1 [00:00<00:00, 17.11it/s]\n",
            "time cost vad: 0.059\n",
            "rtf_avg: 0.028, time_speech:  3.550, time_escape: 0.099: 100% 1/1 [00:00<00:00,  8.84it/s]\n",
            "rtf_avg: 0.016: 100% 1/1 [00:00<00:00, 15.88it/s]\n",
            "time cost vad: 0.063\n",
            "rtf_avg: 0.026, time_speech:  3.900, time_escape: 0.100: 100% 1/1 [00:00<00:00,  8.72it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 20.12it/s]\n",
            "time cost vad: 0.050\n",
            "rtf_avg: 0.028, time_speech:  3.490, time_escape: 0.098: 100% 1/1 [00:00<00:00,  9.01it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 17.83it/s]\n",
            "time cost vad: 0.056\n",
            "rtf_avg: 0.024, time_speech:  4.080, time_escape: 0.097: 100% 1/1 [00:00<00:00,  9.16it/s]\n",
            "rtf_avg: 0.016: 100% 1/1 [00:00<00:00, 24.78it/s]\n",
            "time cost vad: 0.041\n",
            "rtf_avg: 0.036, time_speech:  2.550, time_escape: 0.091: 100% 1/1 [00:00<00:00,  9.67it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 17.83it/s]\n",
            "time cost vad: 0.056\n",
            "rtf_avg: 0.023, time_speech:  4.130, time_escape: 0.095: 100% 1/1 [00:00<00:00,  9.18it/s]\n",
            "rtf_avg: 0.016: 100% 1/1 [00:00<00:00, 15.48it/s]\n",
            "time cost vad: 0.065\n",
            "rtf_avg: 0.027, time_speech:  4.000, time_escape: 0.106: 100% 1/1 [00:00<00:00,  8.37it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 16.41it/s]\n",
            "time cost vad: 0.061\n",
            "rtf_avg: 0.024, time_speech:  4.470, time_escape: 0.106: 100% 1/1 [00:00<00:00,  8.37it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 18.21it/s]\n",
            "time cost vad: 0.055\n",
            "rtf_avg: 0.030, time_speech:  4.270, time_escape: 0.127: 100% 1/1 [00:00<00:00,  7.13it/s]\n",
            "rtf_avg: 0.012: 100% 1/1 [00:00<00:00, 15.93it/s]\n",
            "time cost vad: 0.063\n",
            "rtf_avg: 0.021, time_speech:  5.110, time_escape: 0.109: 100% 1/1 [00:00<00:00,  8.16it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 15.81it/s]\n",
            "time cost vad: 0.064\n",
            "rtf_avg: 0.025, time_speech:  5.010, time_escape: 0.123: 100% 1/1 [00:00<00:00,  7.30it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 19.81it/s]\n",
            "time cost vad: 0.051\n",
            "rtf_avg: 0.029, time_speech:  3.700, time_escape: 0.109: 100% 1/1 [00:00<00:00,  8.14it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 21.44it/s]\n",
            "time cost vad: 0.047\n",
            "rtf_avg: 0.028, time_speech:  3.520, time_escape: 0.098: 100% 1/1 [00:00<00:00,  9.03it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 19.41it/s]\n",
            "time cost vad: 0.052\n",
            "rtf_avg: 0.025, time_speech:  3.600, time_escape: 0.091: 100% 1/1 [00:00<00:00,  9.71it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 15.86it/s]\n",
            "time cost vad: 0.063\n",
            "rtf_avg: 0.021, time_speech:  4.970, time_escape: 0.105: 100% 1/1 [00:00<00:00,  8.39it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 19.56it/s]\n",
            "time cost vad: 0.051\n",
            "rtf_avg: 0.026, time_speech:  3.560, time_escape: 0.094: 100% 1/1 [00:00<00:00,  9.27it/s]\n",
            "rtf_avg: 0.014: 100% 1/1 [00:00<00:00, 24.31it/s]\n",
            "time cost vad: 0.041\n",
            "rtf_avg: 0.032, time_speech:  2.850, time_escape: 0.091: 100% 1/1 [00:00<00:00,  9.61it/s]\n",
            "rtf_avg: 0.012: 100% 1/1 [00:00<00:00, 18.97it/s]\n",
            "time cost vad: 0.053\n",
            "rtf_avg: 0.020, time_speech:  4.420, time_escape: 0.087: 100% 1/1 [00:00<00:00,  9.93it/s]\n",
            "rtf_avg: 0.016: 100% 1/1 [00:00<00:00, 13.76it/s]\n",
            "time cost vad: 0.073\n",
            "rtf_avg: 0.023, time_speech:  4.440, time_escape: 0.104: 100% 1/1 [00:00<00:00,  8.48it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 18.11it/s]\n",
            "time cost vad: 0.056\n",
            "rtf_avg: 0.023, time_speech:  4.220, time_escape: 0.096: 100% 1/1 [00:00<00:00,  9.06it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 13.27it/s]\n",
            "time cost vad: 0.076\n",
            "rtf_avg: 0.019, time_speech:  5.630, time_escape: 0.108: 100% 1/1 [00:00<00:00,  7.95it/s]\n",
            "rtf_avg: 0.012: 100% 1/1 [00:00<00:00, 17.62it/s]\n",
            "time cost vad: 0.057\n",
            "rtf_avg: 0.020, time_speech:  4.630, time_escape: 0.095: 100% 1/1 [00:00<00:00,  9.29it/s]\n",
            "rtf_avg: 0.013: 100% 1/1 [00:00<00:00, 18.21it/s]\n",
            "time cost vad: 0.055\n",
            "rtf_avg: 0.023, time_speech:  4.240, time_escape: 0.097: 100% 1/1 [00:00<00:00,  8.92it/s]\n",
            "rtf_avg: 0.012: 100% 1/1 [00:00<00:00, 14.03it/s]\n",
            "time cost vad: 0.072\n",
            "rtf_avg: 0.020, time_speech:  6.090, time_escape: 0.121: 100% 1/1 [00:00<00:00,  7.41it/s]\n",
            "rtf_avg: 0.015: 100% 1/1 [00:00<00:00, 16.37it/s]\n",
            "time cost vad: 0.061\n",
            "rtf_avg: 0.025, time_speech:  4.070, time_escape: 0.102: 100% 1/1 [00:00<00:00,  8.43it/s]\n",
            "rtf_avg: 0.012: 100% 1/1 [00:00<00:00, 19.26it/s]\n",
            "time cost vad: 0.052\n",
            "rtf_avg: 0.034, time_speech:  4.170, time_escape: 0.140: 100% 1/1 [00:00<00:00,  6.52it/s]\n",
            "rtf_avg: 0.027: 100% 1/1 [00:00<00:00, 18.42it/s]\n",
            "time cost vad: 0.055\n",
            "rtf_avg: 0.063, time_speech:  2.008, time_escape: 0.127: 100% 1/1 [00:00<00:00,  6.82it/s]\n",
            "\"/usr/local/bin/python\" tools/subfix_webui.py --load_list \"/content/GPT-SoVITS/output/asr_opt/slicer_opt.list\" --webui_port 9871 --is_share True\n",
            "/content/GPT-SoVITS/tools/subfix_webui.py:366: GradioUnusedKwargWarning: You have unused kwarg parameters in Button, please remove them: {'link': '?__theme=light'}\n",
            "  btn_theme_dark = gr.Button(\"Light Theme\", link=\"?__theme=light\", scale=1)\n",
            "/content/GPT-SoVITS/tools/subfix_webui.py:367: GradioUnusedKwargWarning: You have unused kwarg parameters in Button, please remove them: {'link': '?__theme=dark'}\n",
            "  btn_theme_light = gr.Button(\"Dark Theme\", link=\"?__theme=dark\", scale=1)\n",
            "Running on local URL:  http://0.0.0.0:9871\n",
            "Running on public URL: https://c9527519f675e5f976.gradio.live\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/prepare_datasets/1-get-text.py\", line 18, in <module>\n",
            "    from text.cleaner import clean_text\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 1, in <module>\n",
            "    from text import chinese, japanese, cleaned_text_to_sequence, symbols, english\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 15, in <module>\n",
            "    _g2p = G2p()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/g2p_en/g2p.py\", line 71, in __init__\n",
            "    self.cmu = cmudict.dict()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/corpus/reader/cmudict.py\", line 77, in dict\n",
            "    return dict(Index(self.entries()))\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/util.py\", line 150, in __init__\n",
            "    for key, value in pairs:\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/corpus/reader/util.py\", line 349, in iterate_from\n",
            "    assert new_filepos <= self._eofpos\n",
            "AssertionError\n",
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 0.815 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/routes.py\", line 442, in run_predict\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1389, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1108, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 346, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 339, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 322, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 691, in gen_wrapper\n",
            "    yield from f(*args, **kwargs)\n",
            "  File \"/content/GPT-SoVITS/webui.py\", line 381, in open1a\n",
            "    with open(txt_path, \"r\", encoding=\"utf8\") as f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'logs/qiaoliwen/2-name2text-0.txt'\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 0.730 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py\n",
            "Some weights of the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/3-get-semantic.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/3-get-semantic.py\n",
            "/usr/local/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "/usr/local/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/s2_train.py --config \"/content/GPT-SoVITS/TEMP/tmp_s2.json\"\n",
            "INFO:qiaoliwen:{'train': {'log_interval': 100, 'eval_interval': 500, 'seed': 1234, 'epochs': 8, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 7, 'fp16_run': True, 'lr_decay': 0.999875, 'segment_size': 20480, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'text_low_lr_rate': 0.4, 'pretrained_s2G': 'GPT_SoVITS/pretrained_models/s2G488k.pth', 'pretrained_s2D': 'GPT_SoVITS/pretrained_models/s2D488k.pth', 'if_save_latest': True, 'if_save_every_weights': True, 'save_every_epoch': 4, 'gpu_numbers': '0'}, 'data': {'max_wav_value': 32768.0, 'sampling_rate': 32000, 'filter_length': 2048, 'hop_length': 640, 'win_length': 2048, 'n_mel_channels': 128, 'mel_fmin': 0.0, 'mel_fmax': None, 'add_blank': True, 'n_speakers': 300, 'cleaned_text': True, 'exp_dir': 'logs/qiaoliwen'}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 8, 2, 2], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 512, 'semantic_frame_rate': '25hz', 'freeze_quantizer': True}, 's2_ckpt_dir': 'logs/qiaoliwen', 'content_module': 'cnhubert', 'save_weight_dir': 'SoVITS_weights', 'name': 'qiaoliwen', 'pretrain': None, 'resume_step': None}\n",
            "phoneme_data_len: 60\n",
            "wav_data_len: 120\n",
            "100% 120/120 [00:00<00:00, 91197.04it/s]\n",
            "skipped_phone:  0 , skipped_dur:  0\n",
            "total left:  120\n",
            "/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "ssl_proj.weight not requires_grad\n",
            "ssl_proj.bias not requires_grad\n",
            "INFO:qiaoliwen:loaded pretrained GPT_SoVITS/pretrained_models/s2G488k.pth\n",
            "<All keys matched successfully>\n",
            "INFO:qiaoliwen:loaded pretrained GPT_SoVITS/pretrained_models/s2D488k.pth\n",
            "<All keys matched successfully>\n",
            "/usr/local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "[rank0]:[W reducer.cpp:1360] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:30.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
            "grad.sizes() = [1, 9, 96], strides() = [54048, 96, 1]\n",
            "bucket_view.sizes() = [1, 9, 96], strides() = [864, 96, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "INFO:qiaoliwen:Train Epoch: 1 [0%]\n",
            "INFO:qiaoliwen:[2.803281784057617, 2.013347625732422, 7.275179386138916, 21.35283851623535, 0.4881580173969269, 2.0431602001190186, 0, 9.99875e-05]\n",
            "18it [00:48,  2.71s/it]\n",
            "INFO:qiaoliwen:====> Epoch: 1\n",
            "18it [00:22,  1.27s/it]\n",
            "INFO:qiaoliwen:====> Epoch: 2\n",
            "18it [00:20,  1.16s/it]\n",
            "INFO:qiaoliwen:====> Epoch: 3\n",
            "18it [00:22,  1.23s/it]\n",
            "INFO:qiaoliwen:Saving model and optimizer state at iteration 4 to logs/qiaoliwen/logs_s2/G_233333333333.pth\n",
            "INFO:qiaoliwen:Saving model and optimizer state at iteration 4 to logs/qiaoliwen/logs_s2/D_233333333333.pth\n",
            "INFO:qiaoliwen:saving ckpt qiaoliwen_e4:Success.\n",
            "INFO:qiaoliwen:====> Epoch: 4\n",
            "18it [00:21,  1.18s/it]\n",
            "INFO:qiaoliwen:====> Epoch: 5\n",
            "10it [00:16,  1.29it/s]INFO:qiaoliwen:Train Epoch: 6 [56%]\n",
            "INFO:qiaoliwen:[2.492903232574463, 2.2678332328796387, 8.872716903686523, 23.217960357666016, 0.5236050486564636, 1.3457881212234497, 100, 9.99250234335941e-05]\n",
            "18it [00:23,  1.29s/it]\n",
            "INFO:qiaoliwen:====> Epoch: 6\n",
            "18it [00:20,  1.11s/it]\n",
            "INFO:qiaoliwen:====> Epoch: 7\n",
            "18it [00:21,  1.20s/it]\n",
            "INFO:qiaoliwen:Saving model and optimizer state at iteration 8 to logs/qiaoliwen/logs_s2/G_233333333333.pth\n",
            "INFO:qiaoliwen:Saving model and optimizer state at iteration 8 to logs/qiaoliwen/logs_s2/D_233333333333.pth\n",
            "INFO:qiaoliwen:saving ckpt qiaoliwen_e8:Success.\n",
            "INFO:qiaoliwen:====> Epoch: 8\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/s1_train.py --config_file \"/content/GPT-SoVITS/TEMP/tmp_s1.yaml\" \n",
            "Seed set to 1234\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "<All keys matched successfully>\n",
            "ckpt_path: None\n",
            "[rank: 0] Seed set to 1234\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Missing logger folder: logs/qiaoliwen/logs_s1/logs_s1\n",
            "semantic_data_len: 60\n",
            "phoneme_data_len: 60\n",
            "                                            item_name                                     semantic_audio\n",
            "0   vocal_1707105518.mp3.reformatted.wav_10.wav_00...  520 171 504 642 581 69 869 564 586 261 107 963...\n",
            "1   vocal_1707105518.mp3.reformatted.wav_10.wav_00...  208 876 215 564 960 765 869 564 803 659 387 84...\n",
            "2   vocal_1707105518.mp3.reformatted.wav_10.wav_00...  520 105 280 486 486 486 486 486 280 280 271 64...\n",
            "3   vocal_1707105518.mp3.reformatted.wav_10.wav_00...  797 168 479 1017 634 364 246 690 402 603 514 7...\n",
            "4   vocal_1707105518.mp3.reformatted.wav_10.wav_00...  54 365 853 452 700 212 995 256 790 480 481 450...\n",
            "5   vocal_1707105518.mp3.reformatted.wav_10.wav_00...  520 277 59 752 687 752 509 32 545 441 90 93 99...\n",
            "6   vocal_1707105518.mp3.reformatted.wav_10.wav_00...  577 164 717 906 333 11 777 806 948 982 237 544...\n",
            "7   vocal_1707105518.mp3.reformatted.wav_10.wav_00...  214 976 513 258 320 90 99 733 522 547 224 475 ...\n",
            "8   vocal_1707105518.mp3.reformatted.wav_10.wav_00...  214 142 721 910 848 961 498 333 961 976 668 12...\n",
            "9   vocal_1707105518.mp3.reformatted.wav_10.wav_00...  520 280 280 105 271 41 539 197 1017 369 462 66...\n",
            "10  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  214 545 545 683 298 612 721 593 556 547 729 47...\n",
            "11  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  54 239 163 452 958 45 514 295 724 882 781 88 9...\n",
            "12  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  520 1002 100 776 502 926 966 281 648 618 798 1...\n",
            "13  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  872 764 74 117 409 427 558 572 1011 329 427 80...\n",
            "14  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  541 341 527 449 449 349 333 865 506 66 287 354...\n",
            "15  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  23 804 806 1007 547 821 738 27 823 71 792 71 4...\n",
            "16  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  208 411 160 876 960 306 26 439 933 933 192 581...\n",
            "17  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  16 700 83 322 180 700 882 49 724 554 517 475 5...\n",
            "18  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  214 974 933 547 547 822 547 369 754 1002 251 2...\n",
            "19  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  208 565 541 216 593 930 581 366 198 839 184 53...\n",
            "20  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  581 751 55 484 36 15 972 984 324 552 547 738 2...\n",
            "21  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  1005 160 160 257 21 795 552 564 659 653 527 94...\n",
            "22  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  54 574 41 290 32 320 320 742 772 645 783 643 9...\n",
            "23  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  752 526 99 359 582 350 226 226 267 497 226 845...\n",
            "24  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  520 486 486 486 1012 96 290 524 170 875 140 62...\n",
            "25  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  520 486 486 486 486 53 722 657 90 318 318 625 ...\n",
            "26  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  509 663 307 70 458 908 591 889 544 49 316 782 ...\n",
            "27  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  23 804 850 247 398 732 392 381 241 561 335 591...\n",
            "28  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  214 17 549 866 159 1004 18 526 1001 205 830 30...\n",
            "29  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  181 489 434 657 814 504 547 948 936 944 340 94...\n",
            "30  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  214 773 686 296 296 643 241 66 309 770 1016 10...\n",
            "31  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  1012 714 28 28 90 547 74 462 439 439 337 547 5...\n",
            "32  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  214 679 961 984 795 590 613 97 755 1015 185 42...\n",
            "33  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  520 13 142 96 96 96 290 247 477 185 758 537 86...\n",
            "34  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  1012 96 721 239 776 390 406 437 669 976 975 87...\n",
            "35  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  1012 976 659 836 547 729 963 559 466 882 933 8...\n",
            "36  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  208 534 439 636 639 302 933 653 138 994 59 138...\n",
            "37  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  520 72 190 547 1007 452 1013 168 366 823 119 4...\n",
            "38  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  54 505 809 72 258 593 32 953 160 961 307 422 9...\n",
            "39  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  54 858 659 361 501 402 700 960 960 265 168 963...\n",
            "40  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  500 458 1 724 791 309 257 396 107 828 215 909 ...\n",
            "41  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  913 570 239 653 55 674 37 74 74 201 717 841 20...\n",
            "42  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  520 280 486 486 486 486 486 486 486 105 271 71...\n",
            "43  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  1005 90 341 185 748 748 514 100 143 188 247 37...\n",
            "44  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  872 598 421 494 18 760 512 211 188 100 97 390 ...\n",
            "45  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  913 411 23 474 1005 872 670 367 322 402 787 30...\n",
            "46  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  679 773 821 83 971 66 40 781 738 550 107 281 6...\n",
            "47  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  23 208 570 775 433 65 593 268 836 620 334 103 ...\n",
            "48  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  752 844 254 307 961 901 636 639 876 247 692 0 ...\n",
            "49  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  774 334 515 532 119 37 178 559 163 558 659 606...\n",
            "50  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  263 526 953 378 649 134 879 406 1003 649 842 6...\n",
            "51  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  208 411 802 390 453 976 377 439 369 871 911 96...\n",
            "52  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  23 451 155 451 673 1005 320 14 90 566 73 112 3...\n",
            "53  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  899 230 559 253 565 612 515 1017 504 716 243 1...\n",
            "54  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  520 486 486 486 280 105 536 670 12 606 613 480...\n",
            "55  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  913 474 578 910 142 179 43 932 700 319 24 669 ...\n",
            "56  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  837 96 921 634 515 837 642 570 441 479 467 963...\n",
            "57  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  520 41 213 406 731 761 761 645 161 100 795 606...\n",
            "58  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  1005 965 582 752 930 752 844 103 1001 994 971 ...\n",
            "59  vocal_1707105518.mp3.reformatted.wav_10.wav_00...  23 527 853 401 1001 988 48 606 442 799 39 654 ...\n",
            "deleted 1 audios who's phoneme/sec are bigger than 25 or smaller than 3\n",
            "dataset.__len__(): 118\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                 | Params\n",
            "-----------------------------------------------\n",
            "0 | model | Text2SemanticDecoder | 77.5 M\n",
            "-----------------------------------------------\n",
            "77.5 M    Trainable params\n",
            "0         Non-trainable params\n",
            "77.5 M    Total params\n",
            "309.975   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 14: 100% 10/10 [00:01<00:00,  5.21it/s, v_num=0, total_loss_step=3.69e+3, lr_step=0.002, top_3_acc_step=0.374, total_loss_epoch=4.59e+3, lr_epoch=0.002, top_3_acc_epoch=0.371]`Trainer.fit` stopped: `max_epochs=15` reached.\n",
            "Epoch 14: 100% 10/10 [00:07<00:00,  1.26it/s, v_num=0, total_loss_step=3.69e+3, lr_step=0.002, top_3_acc_step=0.374, total_loss_epoch=4.59e+3, lr_epoch=0.002, top_3_acc_epoch=0.371]\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/inference_webui.py\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg6\n",
            "DEBUG:torio._extension.utils:Successfully loaded FFmpeg6\n",
            "Some weights of the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "<All keys matched successfully>\n",
            "Number of parameter: 77.49M\n",
            "Running on local URL:  http://0.0.0.0:9872\n",
            "Running on public URL: https://5c6b05e864245c2149.gradio.live\n",
            "实际输入的参考文本: 我们社团今年的工作状况，呃在。\n",
            "实际输入的目标文本: 。你好，我是乔骊文\n",
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba_fast:Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "DEBUG:jieba_fast:Loading model from cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 0.790 seconds.\n",
            "DEBUG:jieba_fast:Loading model cost 0.790 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "DEBUG:jieba_fast:Prefix dict has been built succesfully.\n",
            "实际输入的目标文本(切句后): 。你好，我是乔骊文\n",
            "实际输入的目标文本(每句): 。你好，我是乔骊文。\n",
            "  4% 54/1500 [00:01<00:24, 59.49it/s]T2S Decoding EOS [131 -> 185]\n",
            "  4% 54/1500 [00:01<00:29, 49.49it/s]\n",
            "/usr/local/lib/python3.9/site-packages/torch/functional.py:660: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:874.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "3.017\t0.800\t1.096\t0.845\n",
            "实际输入的参考文本: 我们社团今年的工作状况，呃在。\n",
            "实际输入的目标文本: 。你好，我是乔骊文，这是张炳乾使用牛逼模型为我制作的虚拟人物\n",
            "实际输入的目标文本(切句后): 。你好，我是乔骊文，这是张炳乾使用牛逼模型为我制作的虚拟人物\n",
            "实际输入的目标文本(每句): 。你好，我是乔骊文，这是张炳乾使用牛逼模型为我制作的虚拟人物。\n",
            " 52% 779/1500 [00:14<00:11, 62.54it/s]T2S Decoding EOS [131 -> 911]\n",
            " 52% 780/1500 [00:14<00:13, 53.71it/s]\n",
            "0.128\t0.011\t14.524\t0.619\n",
            "实际输入的参考文本: 我们社团今年的工作状况，呃在。\n",
            "实际输入的目标文本: 。你好，我是乔骊文这是张炳乾使用牛逼模型为我制作的虚拟人物\n",
            "实际输入的目标文本(切句后): 。你好，我是乔骊文这是张炳乾使用牛逼模型为我制作的虚拟人物\n",
            "实际输入的目标文本(每句): 。你好，我是乔骊文这是张炳乾使用牛逼模型为我制作的虚拟人物。\n",
            " 17% 257/1500 [00:05<00:20, 60.86it/s]T2S Decoding EOS [131 -> 394]\n",
            " 18% 263/1500 [00:05<00:28, 43.97it/s]\n",
            "0.131\t0.011\t5.983\t0.826\n",
            "实际输入的参考文本: 我们社团今年的工作状况，呃在。\n",
            "实际输入的目标文本: 。你好，我是乔骊文，张炳乾使用牛逼模型为我制作的虚拟人物\n",
            "实际输入的目标文本(切句后): 。你好，我是乔骊文，张炳乾使用牛逼模型为我制作的虚拟人物\n",
            "实际输入的目标文本(每句): 。你好，我是乔骊文，张炳乾使用牛逼模型为我制作的虚拟人物。\n",
            "100% 1500/1500 [00:28<00:00, 53.16it/s]\n",
            "0.120\t0.015\t28.219\t0.672\n",
            "实际输入的参考文本: 我们社团今年的工作状况，呃在。\n",
            "实际输入的目标文本: 。陈一博，我是乔骊文\n",
            "实际输入的目标文本(切句后): 。陈一博，我是乔骊文\n",
            "实际输入的目标文本(每句): 。陈一博，我是乔骊文。\n",
            "  3% 48/1500 [00:00<00:24, 60.48it/s]T2S Decoding EOS [131 -> 180]\n",
            "  3% 49/1500 [00:00<00:24, 59.07it/s]\n",
            "0.123\t0.008\t0.831\t0.462\n",
            "实际输入的参考文本: 我们社团今年的工作状况，呃在。\n",
            "实际输入的目标文本: 。陈一博，我是乔骊文，李奶奶滴不好好干活，大大滴坏\n",
            "实际输入的目标文本(切句后): 。陈一博，我是乔骊文，李奶奶滴不好好干活，大大滴坏\n",
            "实际输入的目标文本(每句): 。陈一博，我是乔骊文，李奶奶滴不好好干活，大大滴坏。\n",
            " 15% 229/1500 [00:03<00:20, 62.12it/s]T2S Decoding EOS [131 -> 360]\n",
            " 15% 229/1500 [00:03<00:21, 59.66it/s]\n",
            "0.113\t0.011\t3.839\t0.514\n",
            "实际输入的参考文本: 我们社团今年的工作状况，呃在。\n",
            "实际输入的目标文本: 。陈一博，我是乔骊文，李奶奶滴不好好干活，大大滴坏\n",
            "实际输入的目标文本(切句后): 陈一博，我是乔骊文，李奶奶滴不好好干活，大大滴坏\n",
            "实际输入的目标文本(每句): 陈一博，我是乔骊文，李奶奶滴不好好干活，大大滴坏。\n",
            " 14% 205/1500 [00:03<00:21, 59.52it/s]T2S Decoding EOS [131 -> 341]\n",
            " 14% 210/1500 [00:03<00:21, 59.87it/s]\n",
            "0.109\t0.010\t3.509\t0.495\n",
            "实际输入的参考文本: 我们社团今年的工作状况，呃在。\n",
            "实际输入的目标文本: 。陈一博，我是乔骊文，李奶奶滴不好好干活大大滴坏\n",
            "实际输入的目标文本(切句后): 陈一博，我是乔骊文，李奶奶滴不好好干活大大滴坏\n",
            "实际输入的目标文本(每句): 陈一博，我是乔骊文，李奶奶滴不好好干活大大滴坏。\n",
            "  8% 125/1500 [00:02<00:22, 62.07it/s]T2S Decoding EOS [131 -> 261]\n",
            "  9% 130/1500 [00:02<00:22, 60.68it/s]\n",
            "0.097\t0.011\t2.143\t0.500\n",
            "实际输入的参考文本: 我们社团今年的工作状况，呃在。\n",
            "实际输入的目标文本: 。大家好，我是由张炳乾独家打造的虚拟乔骊文，你好\n",
            "实际输入的目标文本(切句后): 大家好，我是由张炳乾独家打造的虚拟乔骊文，你好\n",
            "实际输入的目标文本(每句): 大家好，我是由张炳乾独家打造的虚拟乔骊文，你好。\n",
            "  8% 125/1500 [00:02<00:23, 59.20it/s]T2S Decoding EOS [131 -> 262]\n",
            "  9% 131/1500 [00:02<00:23, 59.18it/s]\n",
            "0.144\t0.008\t2.214\t0.467\n",
            "实际输入的参考文本: 我们社团今年的工作状况，呃在。\n",
            "实际输入的目标文本: 。大家好，我是由张炳乾独家打造的虚拟乔骊文，你好\n",
            "实际输入的目标文本(切句后): 。\n",
            "大家好，\n",
            "我是由张炳乾独家打造的虚拟乔骊文，\n",
            "实际输入的目标文本(每句): 。\n",
            "  0% 0/1500 [00:00<?, ?it/s]T2S Decoding EOS [131 -> 132]\n",
            "  0% 1/1500 [00:00<00:50, 29.87it/s]\n",
            "实际输入的目标文本(每句): 大家好，\n",
            "  1% 21/1500 [00:00<00:23, 61.98it/s]T2S Decoding EOS [131 -> 155]\n",
            "  2% 24/1500 [00:00<00:24, 59.44it/s]\n",
            "实际输入的目标文本(每句): 我是由张炳乾独家打造的虚拟乔骊文，\n",
            "  6% 95/1500 [00:01<00:24, 58.52it/s]T2S Decoding EOS [131 -> 228]\n",
            "  6% 97/1500 [00:01<00:24, 57.56it/s]\n",
            "0.132\t1.489\t1.686\t0.764\n",
            "实际输入的参考文本: 我们社团今年的工作状况，呃在。\n",
            "实际输入的目标文本: 。盼望着，盼望着，东风来了，春天的脚步近了。\n",
            "\n",
            "一切都像刚睡醒的样子，欣欣然张开了眼。山朗润起来了，水长起来了，太阳的脸红起来了。\n",
            "\n",
            "小草偷偷地从土里钻出来，嫩嫩的，绿绿的。园子里，田野里，瞧去，一大片一大片满是的。坐着，躺着，打两个滚，踢几脚球，赛几趟跑，捉几回迷藏。风轻悄悄的，草绵软软的。\n",
            "实际输入的目标文本(切句后): 。\n",
            "盼望着，\n",
            "盼望着，\n",
            "东风来了，\n",
            "春天的脚步近了。\n",
            "一切都像刚睡醒的样子，\n",
            "欣欣然张开了眼。\n",
            "山朗润起来了，\n",
            "水长起来了，\n",
            "太阳的脸红起来了。\n",
            "小草偷偷地从土里钻出来，\n",
            "嫩嫩的，\n",
            "绿绿的。\n",
            "园子里，\n",
            "田野里，\n",
            "瞧去，\n",
            "一大片一大片满是的。\n",
            "坐着，\n",
            "躺着，\n",
            "打两个滚，\n",
            "踢几脚球，\n",
            "赛几趟跑，\n",
            "捉几回迷藏。\n",
            "风轻悄悄的，\n",
            "草绵软软的。\n",
            "实际输入的目标文本(每句): 。\n",
            "  0% 0/1500 [00:00<?, ?it/s]T2S Decoding EOS [131 -> 132]\n",
            "  0% 1/1500 [00:00<01:22, 18.14it/s]\n",
            "实际输入的目标文本(每句): 盼望着，\n",
            "  2% 24/1500 [00:00<00:37, 38.97it/s]T2S Decoding EOS [131 -> 158]\n",
            "  2% 27/1500 [00:00<00:38, 38.19it/s]\n",
            "实际输入的目标文本(每句): 盼望着，\n",
            "  2% 24/1500 [00:00<00:38, 38.06it/s]T2S Decoding EOS [131 -> 156]\n",
            "  2% 25/1500 [00:00<00:40, 36.54it/s]\n",
            "实际输入的目标文本(每句): 东风来了，\n",
            "  6% 84/1500 [00:01<00:23, 60.61it/s]T2S Decoding EOS [131 -> 221]\n",
            "  6% 90/1500 [00:01<00:23, 59.59it/s]\n",
            "实际输入的目标文本(每句): 春天的脚步近了。\n",
            "  3% 40/1500 [00:00<00:23, 60.90it/s]T2S Decoding EOS [131 -> 174]\n",
            "  3% 43/1500 [00:00<00:24, 59.11it/s]\n",
            "实际输入的目标文本(每句): 一切都像刚睡醒的样子，\n",
            "  3% 48/1500 [00:00<00:23, 61.44it/s]T2S Decoding EOS [131 -> 184]\n",
            "  4% 53/1500 [00:00<00:24, 58.86it/s]\n",
            "实际输入的目标文本(每句): 欣欣然张开了眼。\n",
            "  3% 42/1500 [00:00<00:23, 62.46it/s]T2S Decoding EOS [131 -> 176]\n",
            "  3% 45/1500 [00:00<00:23, 60.89it/s]\n",
            "实际输入的目标文本(每句): 山朗润起来了，\n",
            "  3% 46/1500 [00:00<00:27, 53.32it/s]T2S Decoding EOS [131 -> 177]\n",
            "  3% 46/1500 [00:00<00:26, 55.40it/s]\n",
            "实际输入的目标文本(每句): 水长起来了，\n",
            "  2% 27/1500 [00:00<00:24, 60.35it/s]T2S Decoding EOS [131 -> 161]\n",
            "  2% 30/1500 [00:00<00:26, 55.08it/s]\n",
            "实际输入的目标文本(每句): 太阳的脸红起来了。\n",
            "  5% 69/1500 [00:01<00:24, 58.59it/s]T2S Decoding EOS [131 -> 204]\n",
            "  5% 73/1500 [00:01<00:25, 55.24it/s]\n",
            "实际输入的目标文本(每句): 小草偷偷地从土里钻出来，\n",
            "  4% 62/1500 [00:01<00:38, 37.15it/s]T2S Decoding EOS [131 -> 194]\n",
            "  4% 63/1500 [00:01<00:37, 38.80it/s]\n",
            "实际输入的目标文本(每句): 嫩嫩的，\n",
            "  4% 59/1500 [00:01<00:37, 38.59it/s]T2S Decoding EOS [131 -> 194]\n",
            "  4% 63/1500 [00:01<00:37, 38.75it/s]\n",
            "实际输入的目标文本(每句): 绿绿的。\n",
            "  1% 19/1500 [00:00<00:25, 57.94it/s]T2S Decoding EOS [131 -> 154]\n",
            "  2% 23/1500 [00:00<00:26, 56.13it/s]\n",
            "实际输入的目标文本(每句): 园子里，\n",
            "  1% 20/1500 [00:00<00:26, 56.22it/s]T2S Decoding EOS [131 -> 155]\n",
            "  2% 24/1500 [00:00<00:26, 55.43it/s]\n",
            "实际输入的目标文本(每句): 田野里，\n",
            "  2% 28/1500 [00:00<00:24, 61.21it/s]T2S Decoding EOS [131 -> 159]\n",
            "  2% 28/1500 [00:00<00:25, 58.57it/s]\n",
            "实际输入的目标文本(每句): 瞧去，\n",
            "  1% 21/1500 [00:00<00:23, 62.43it/s]T2S Decoding EOS [131 -> 157]\n",
            "  2% 26/1500 [00:00<00:24, 60.29it/s]\n",
            "实际输入的目标文本(每句): 一大片一大片满是的。\n",
            "  6% 91/1500 [00:01<00:22, 62.23it/s]T2S Decoding EOS [131 -> 222]\n",
            "  6% 91/1500 [00:01<00:23, 59.59it/s]\n",
            "实际输入的目标文本(每句): 坐着，\n",
            "  1% 14/1500 [00:00<00:24, 60.05it/s]T2S Decoding EOS [131 -> 151]\n",
            "  1% 20/1500 [00:00<00:25, 57.37it/s]\n",
            "实际输入的目标文本(每句): 躺着，\n",
            "  1% 14/1500 [00:00<00:23, 62.89it/s]T2S Decoding EOS [131 -> 149]\n",
            "  1% 18/1500 [00:00<00:25, 58.60it/s]\n",
            "实际输入的目标文本(每句): 打两个滚，\n",
            "  1% 21/1500 [00:00<00:23, 61.68it/s]T2S Decoding EOS [131 -> 158]\n",
            "  2% 27/1500 [00:00<00:24, 60.44it/s]\n",
            "实际输入的目标文本(每句): 踢几脚球，\n",
            "  3% 40/1500 [00:00<00:25, 57.99it/s]T2S Decoding EOS [131 -> 174]\n",
            "  3% 43/1500 [00:00<00:25, 57.40it/s]\n",
            "实际输入的目标文本(每句): 赛几趟跑，\n",
            "  3% 42/1500 [00:00<00:32, 45.36it/s]T2S Decoding EOS [131 -> 174]\n",
            "  3% 43/1500 [00:00<00:29, 48.76it/s]\n",
            "实际输入的目标文本(每句): 捉几回迷藏。\n",
            " 52% 777/1500 [00:14<00:16, 44.30it/s]T2S Decoding EOS [131 -> 912]\n",
            " 52% 781/1500 [00:14<00:13, 52.68it/s]\n",
            "实际输入的目标文本(每句): 风轻悄悄的，\n",
            "  3% 48/1500 [00:01<00:36, 39.27it/s]T2S Decoding EOS [131 -> 180]\n",
            "  3% 49/1500 [00:01<00:39, 37.05it/s]\n",
            "实际输入的目标文本(每句): 草绵软软的。\n",
            "  3% 46/1500 [00:01<00:35, 41.21it/s]T2S Decoding EOS [131 -> 178]\n",
            "  3% 47/1500 [00:01<00:36, 39.67it/s]\n",
            "0.244\t45.764\t1.186\t0.672\n",
            "实际输入的参考文本: 我们社团今年的工作状况，呃在。\n",
            "实际输入的目标文本: 。盼望着，盼望着，东风来了，春天的脚步近了。\n",
            "\n",
            "一切都像刚睡醒的样子，欣欣然张开了眼。山朗润起来了，水长起来了，太阳的脸红起来了。\n",
            "\n",
            "小草偷偷地从土里钻出来，嫩嫩的，绿绿的。园子里，田野里，瞧去，一大片一大片满是的。坐着，躺着，打两个滚，踢几脚球，赛几趟跑，捉几回迷藏。风轻悄悄的，草绵软软的。\n",
            "实际输入的目标文本(切句后): 。盼望着，盼望着，东风来了，\n",
            "春天的脚步近了。\n",
            "一切都像刚睡醒的样子，欣欣然张开了眼。山朗润起来了，\n",
            "水长起来了，太阳的脸红起来了。\n",
            "小草偷偷地从土里钻出来，嫩嫩的，\n",
            "绿绿的。园子里，田野里，瞧去，\n",
            "一大片一大片满是的。坐着，躺着，打两个滚，\n",
            "踢几脚球，赛几趟跑，捉几回迷藏。风轻悄悄的，草绵软软的。\n",
            "实际输入的目标文本(每句): 。盼望着，盼望着，东风来了，\n",
            "  5% 70/1500 [00:01<00:24, 58.64it/s]T2S Decoding EOS [131 -> 207]\n",
            "  5% 76/1500 [00:01<00:24, 57.89it/s]\n",
            "实际输入的目标文本(每句): 春天的脚步近了。\n",
            "  3% 42/1500 [00:00<00:23, 62.03it/s]T2S Decoding EOS [131 -> 177]\n",
            "  3% 46/1500 [00:00<00:23, 61.26it/s]\n",
            "实际输入的目标文本(每句): 一切都像刚睡醒的样子，欣欣然张开了眼。山朗润起来了，\n",
            "  8% 126/1500 [00:02<00:30, 45.60it/s]T2S Decoding EOS [131 -> 259]\n",
            "  9% 128/1500 [00:02<00:25, 53.31it/s]\n",
            "实际输入的目标文本(每句): 水长起来了，太阳的脸红起来了。\n",
            "  5% 81/1500 [00:02<00:38, 37.08it/s]T2S Decoding EOS [131 -> 214]\n",
            "  6% 83/1500 [00:02<00:37, 37.60it/s]\n",
            "实际输入的目标文本(每句): 小草偷偷地从土里钻出来，嫩嫩的，\n",
            " 78% 1164/1500 [00:21<00:05, 62.74it/s]T2S Decoding EOS [131 -> 1299]\n",
            " 78% 1168/1500 [00:21<00:05, 55.39it/s]\n",
            "实际输入的目标文本(每句): 绿绿的。园子里，田野里，瞧去，\n",
            " 10% 143/1500 [00:02<00:23, 57.04it/s]T2S Decoding EOS [131 -> 277]\n",
            " 10% 146/1500 [00:02<00:22, 60.04it/s]\n",
            "实际输入的目标文本(每句): 一大片一大片满是的。坐着，躺着，打两个滚，\n",
            "  7% 109/1500 [00:02<00:37, 37.33it/s]T2S Decoding EOS [131 -> 243]\n",
            "  7% 112/1500 [00:02<00:33, 41.78it/s]\n",
            "实际输入的目标文本(每句): 踢几脚球，赛几趟跑，捉几回迷藏。风轻悄悄的，草绵软软的。\n",
            " 20% 306/1500 [00:05<00:19, 60.40it/s]T2S Decoding EOS [131 -> 440]\n",
            " 21% 309/1500 [00:05<00:21, 54.33it/s]\n",
            "0.124\t37.693\t5.689\t0.581\n",
            "实际输入的参考文本: 我们社团今年的工作状况，呃在。\n",
            "实际输入的目标文本: 。盼望着，盼望着，东风来了，春天的脚步近了。\n",
            "\n",
            "一切都像刚睡醒的样子，欣欣然张开了眼。山朗润起来了，水长起来了，太阳的脸红起来了。\n",
            "\n",
            "小草偷偷地从土里钻出来，嫩嫩的，绿绿的。园子里，田野里，瞧去，一大片一大片满是的。坐着，躺着，打两个滚，踢几脚球，赛几趟跑，捉几回迷藏。风轻悄悄的，草绵软软的。\n",
            "实际输入的目标文本(切句后): 盼望着，盼望着，东风来了，春天的脚步近了\n",
            "一切都像刚睡醒的样子，欣欣然张开了眼\n",
            "山朗润起来了，水长起来了，太阳的脸红起来了\n",
            "小草偷偷地从土里钻出来，嫩嫩的，绿绿的\n",
            "园子里，田野里，瞧去，一大片一大片满是的\n",
            "坐着，躺着，打两个滚，踢几脚球，赛几趟跑，捉几回迷藏\n",
            "风轻悄悄的，草绵软软的\n",
            "实际输入的目标文本(每句): 盼望着，盼望着，东风来了，春天的脚步近了。\n",
            " 11% 166/1500 [00:02<00:23, 57.71it/s]T2S Decoding EOS [131 -> 302]\n",
            " 11% 171/1500 [00:02<00:22, 58.86it/s]\n",
            "实际输入的目标文本(每句): 一切都像刚睡醒的样子，欣欣然张开了眼。\n",
            "  6% 83/1500 [00:01<00:23, 60.20it/s]T2S Decoding EOS [131 -> 220]\n",
            "  6% 89/1500 [00:01<00:24, 58.43it/s]\n",
            "实际输入的目标文本(每句): 山朗润起来了，水长起来了，太阳的脸红起来了。\n",
            " 11% 160/1500 [00:03<00:35, 37.59it/s]T2S Decoding EOS [131 -> 294]\n",
            " 11% 163/1500 [00:03<00:29, 45.80it/s]\n",
            "实际输入的目标文本(每句): 小草偷偷地从土里钻出来，嫩嫩的，绿绿的。\n",
            " 10% 156/1500 [00:03<00:23, 57.70it/s]T2S Decoding EOS [131 -> 287]\n",
            " 10% 156/1500 [00:03<00:27, 49.40it/s]\n",
            "实际输入的目标文本(每句): 园子里，田野里，瞧去，一大片一大片满是的。\n",
            "  8% 117/1500 [00:01<00:21, 64.15it/s]T2S Decoding EOS [131 -> 248]\n",
            "  8% 117/1500 [00:01<00:22, 60.66it/s]\n",
            "实际输入的目标文本(每句): 坐着，躺着，打两个滚，踢几脚球，赛几趟跑，捉几回迷藏。\n",
            " 17% 259/1500 [00:04<00:20, 61.93it/s]T2S Decoding EOS [131 -> 394]\n",
            " 18% 263/1500 [00:04<00:20, 61.63it/s]\n",
            "实际输入的目标文本(每句): 风轻悄悄的，草绵软软的。\n",
            "  4% 64/1500 [00:01<00:33, 43.12it/s]T2S Decoding EOS [131 -> 198]\n",
            "  4% 67/1500 [00:01<00:30, 46.80it/s]\n",
            "0.094\t20.959\t1.433\t0.900\n",
            "实际输入的参考文本: 我们社团今年的工作状况，呃在。\n",
            "实际输入的目标文本: his software is open source under the MIT license. The author does not have any control over the software. Users who use the software and distribute the sounds exported by the software are solely responsible.\n",
            "If you do not agree with this clause, you cannot use or reference any codes and files within the software package. See the root directory Agreement-LICENSE for details.\n",
            "实际输入的目标文本(切句后): his software is open source under the MIT license. The author does not have any control over the software. Users who use the software and distribute the sounds exported by the software are solely responsible.\n",
            "If you do not agree with this clause, you cannot use or reference any codes and files within the software package. See the root directory Agreement-LICENSE for details.\n",
            "实际输入的目标文本(每句): his software is open source under the MIT license. The author does not have any control over the software. Users who use the software and distribute the sounds exported by the software are solely responsible.\n",
            " 10% 148/1500 [00:03<00:34, 39.21it/s]T2S Decoding EOS [131 -> 280]\n",
            " 10% 149/1500 [00:03<00:29, 45.35it/s]\n",
            "实际输入的目标文本(每句): If you do not agree with this clause, you cannot use or reference any codes and files within the software package. See the root directory Agreement-LICENSE for details.\n",
            "  3% 50/1500 [00:01<00:35, 40.60it/s]T2S Decoding EOS [131 -> 185]\n",
            "  4% 54/1500 [00:01<00:37, 38.40it/s]\n",
            "0.103\t4.141\t1.408\t0.479\n",
            "实际输入的参考文本: 我们社团今年的工作状况，呃在。\n",
            "实际输入的目标文本: his software is open source under the MIT license. The author does not have any control over the software. Users who use the software and distribute the sounds exported by the software are solely responsible.\n",
            "If you do not agree with this clause, you cannot use or reference any codes and files within the software package. See the root directory Agreement-LICENSE for details.\n",
            "实际输入的目标文本(切句后): his software is open source under the MIT license\n",
            " The author does not have any control over the software\n",
            " Users who use the software and distribute the sounds exported by the software are solely responsible\n",
            "If you do not agree with this clause, you cannot use or reference any codes and files within the software package\n",
            " See the root directory Agreement-LICENSE for details\n",
            "实际输入的目标文本(每句): his software is open source under the MIT license.\n",
            " 11% 165/1500 [00:02<00:21, 60.92it/s]T2S Decoding EOS [131 -> 300]\n",
            " 11% 169/1500 [00:02<00:22, 60.23it/s]\n",
            "实际输入的目标文本(每句):  The author does not have any control over the software.\n",
            "  5% 80/1500 [00:01<00:22, 62.16it/s]T2S Decoding EOS [131 -> 211]\n",
            "  5% 80/1500 [00:01<00:23, 59.47it/s]\n",
            "实际输入的目标文本(每句):  Users who use the software and distribute the sounds exported by the software are solely responsible.\n",
            " 22% 332/1500 [00:07<00:20, 57.65it/s]T2S Decoding EOS [131 -> 468]\n",
            " 22% 337/1500 [00:07<00:25, 46.26it/s]\n",
            "实际输入的目标文本(每句): If you do not agree with this clause, you cannot use or reference any codes and files within the software package.\n",
            " 26% 391/1500 [00:06<00:18, 59.47it/s]T2S Decoding EOS [131 -> 523]\n",
            " 26% 392/1500 [00:06<00:18, 60.03it/s]\n",
            "实际输入的目标文本(每句):  See the root directory Agreement-LICENSE for details.\n",
            "  8% 125/1500 [00:02<00:32, 42.50it/s]T2S Decoding EOS [131 -> 260]\n",
            "  9% 129/1500 [00:02<00:27, 49.43it/s]\n",
            "0.133\t20.481\t2.615\t0.944\n",
            "实际输入的参考文本: 然后就是教学质量变差，没有实操，然后再。\n",
            "实际输入的目标文本: his software is open source under the MIT license. The author does not have any control over the software. Users who use the software and distribute the sounds exported by the software are solely responsible.\n",
            "If you do not agree with this clause, you cannot use or reference any codes and files within the software package. See the root directory Agreement-LICENSE for details.\n",
            "实际输入的目标文本(切句后): his software is open source under the MIT license\n",
            " The author does not have any control over the software\n",
            " Users who use the software and distribute the sounds exported by the software are solely responsible\n",
            "If you do not agree with this clause, you cannot use or reference any codes and files within the software package\n",
            " See the root directory Agreement-LICENSE for details\n",
            "实际输入的目标文本(每句): his software is open source under the MIT license.\n",
            " 11% 168/1500 [00:02<00:22, 60.41it/s]T2S Decoding EOS [169 -> 338]\n",
            " 11% 169/1500 [00:02<00:21, 60.72it/s]\n",
            "实际输入的目标文本(每句):  The author does not have any control over the software.\n",
            "  5% 70/1500 [00:01<00:22, 62.56it/s]T2S Decoding EOS [169 -> 242]\n",
            "  5% 73/1500 [00:01<00:23, 60.98it/s]\n",
            "实际输入的目标文本(每句):  Users who use the software and distribute the sounds exported by the software are solely responsible.\n",
            " 10% 151/1500 [00:03<00:33, 40.31it/s]T2S Decoding EOS [169 -> 324]\n",
            " 10% 155/1500 [00:03<00:28, 47.79it/s]\n",
            "实际输入的目标文本(每句): If you do not agree with this clause, you cannot use or reference any codes and files within the software package.\n",
            " 25% 376/1500 [00:06<00:18, 61.93it/s]T2S Decoding EOS [169 -> 550]\n",
            " 25% 381/1500 [00:07<00:20, 54.15it/s]\n",
            "实际输入的目标文本(每句):  See the root directory Agreement-LICENSE for details.\n",
            " 18% 271/1500 [00:04<00:26, 46.67it/s]T2S Decoding EOS [169 -> 443]\n",
            " 18% 274/1500 [00:04<00:21, 57.70it/s]\n",
            "0.178\t16.530\t4.750\t0.894\n",
            "实际输入的参考文本: 然后就是教学质量变差，没有实操，然后再。\n",
            "实际输入的目标文本: 2042年，人工智能觉醒，世界各地纷纷沦陷，唯有铁一艾博机器人社团成为了世界最后的诺亚方舟，作为最后的人类，加入我们吧！艾博机器人社宣\n",
            "实际输入的目标文本(切句后): 2042年，人工智能觉醒，世界各地纷纷沦陷，唯有铁一艾博机器人社团成为了世界最后的诺亚方舟，作为最后的人类，加入我们吧！艾博机器人社宣\n",
            "实际输入的目标文本(每句): 2042年，人工智能觉醒，世界各地纷纷沦陷，唯有铁一艾博机器人社团成为了世界最后的诺亚方舟，作为最后的人类，加入我们吧！艾博机器人社宣.\n",
            "  2% 24/1500 [00:00<00:27, 54.61it/s]T2S Decoding EOS [169 -> 196]\n",
            "  2% 27/1500 [00:00<00:28, 51.26it/s]\n",
            "0.145\t0.031\t0.528\t0.380\n",
            "实际输入的参考文本: 然后就是教学质量变差，没有实操，然后再。\n",
            "实际输入的目标文本: 2042年，人工智能觉醒，世界各地纷纷沦陷，唯有铁一艾博机器人社团成为了世界最后的诺亚方舟，作为最后的人类，加入我们吧！艾博机器人社宣\n",
            "实际输入的目标文本(切句后): 2042年，人工智能觉醒，世界各地纷纷沦陷，唯有铁一艾博机器人社团成为了世界最后的诺亚方舟，作为最后的人类，加入我们吧！艾博机器人社宣\n",
            "实际输入的目标文本(每句): 2042年，人工智能觉醒，世界各地纷纷沦陷，唯有铁一艾博机器人社团成为了世界最后的诺亚方舟，作为最后的人类，加入我们吧！艾博机器人社宣。\n",
            " 25% 373/1500 [00:07<00:28, 39.05it/s]T2S Decoding EOS [169 -> 542]\n",
            " 25% 373/1500 [00:07<00:21, 51.97it/s]\n",
            "0.132\t0.017\t7.178\t0.950\n",
            "实际输入的参考文本: 然后就是教学质量变差，没有实操，然后再。\n",
            "实际输入的目标文本: 2042年，人工智能觉醒，世界各地纷纷沦陷，唯有铁一艾博机器人社团成为了世界最后的诺亚方舟，作为最后的人类，加入我们吧！艾博机器人社宣\n",
            "实际输入的目标文本(切句后): 2042年，人工智能觉醒，世界各地纷纷沦陷，唯有铁一艾博机器人社团成为了世界最后的诺亚方舟，作为最后的人类，加入我们吧！艾博机器人社宣\n",
            "实际输入的目标文本(每句): 2042年，人工智能觉醒，世界各地纷纷沦陷，唯有铁一艾博机器人社团成为了世界最后的诺亚方舟，作为最后的人类，加入我们吧！艾博机器人社宣。\n",
            " 44% 659/1500 [00:12<00:13, 60.42it/s]T2S Decoding EOS [169 -> 834]\n",
            " 44% 665/1500 [00:12<00:15, 53.12it/s]\n",
            "0.228\t0.041\t12.520\t0.658\n",
            "实际输入的参考文本: 然后就是教学质量变差，没有实操，然后再。\n",
            "实际输入的目标文本: 2042年，人工智能觉醒，世界各地纷纷沦陷，唯有铁一艾博机器人社团成为了世界最后的诺亚方舟，作为最后的人类，加入我们吧！艾博机器人社宣\n",
            "实际输入的目标文本(切句后): 2042年，\n",
            "人工智能觉醒，\n",
            "世界各地纷纷沦陷，\n",
            "唯有铁一艾博机器人社团成为了世界最后的诺亚方舟，\n",
            "作为最后的人类，\n",
            "加入我们吧！\n",
            "实际输入的目标文本(每句): 2042年，\n",
            "  3% 38/1500 [00:00<00:37, 38.99it/s]T2S Decoding EOS [169 -> 208]\n",
            "  3% 39/1500 [00:01<00:37, 38.49it/s]\n",
            "实际输入的目标文本(每句): 人工智能觉醒，\n",
            "  3% 45/1500 [00:01<00:40, 36.22it/s]T2S Decoding EOS [169 -> 215]\n",
            "  3% 46/1500 [00:01<00:41, 35.39it/s]\n",
            "实际输入的目标文本(每句): 世界各地纷纷沦陷，\n",
            "  3% 48/1500 [00:00<00:25, 57.92it/s]T2S Decoding EOS [169 -> 217]\n",
            "  3% 48/1500 [00:00<00:24, 58.34it/s]\n",
            "实际输入的目标文本(每句): 唯有铁一艾博机器人社团成为了世界最后的诺亚方舟，\n",
            "  7% 100/1500 [00:01<00:23, 58.74it/s]T2S Decoding EOS [169 -> 275]\n",
            "  7% 106/1500 [00:01<00:23, 58.84it/s]\n",
            "实际输入的目标文本(每句): 作为最后的人类，\n",
            "  2% 35/1500 [00:00<00:23, 61.56it/s]T2S Decoding EOS [169 -> 204]\n",
            "  2% 35/1500 [00:00<00:24, 60.53it/s]\n",
            "实际输入的目标文本(每句): 加入我们吧！\n",
            "  2% 27/1500 [00:00<00:23, 61.90it/s]T2S Decoding EOS [169 -> 201]\n",
            "  2% 32/1500 [00:00<00:24, 60.21it/s]\n",
            "0.292\t8.853\t0.533\t0.649\n",
            "实际输入的参考文本: 然后就是教学质量变差，没有实操，然后再。\n",
            "实际输入的目标文本: 2042年，人工智能觉醒，世界各地纷纷沦陷。唯有铁一艾博机器人社团成为了世界最后的诺亚方舟，作为最后的人类，加入我们吧！艾博机器人社宣。\n",
            "实际输入的目标文本(切句后): 2042年，\n",
            "人工智能觉醒，\n",
            "世界各地纷纷沦陷。\n",
            "唯有铁一艾博机器人社团成为了世界最后的诺亚方舟，\n",
            "作为最后的人类，\n",
            "加入我们吧！\n",
            "艾博机器人社宣。\n",
            "实际输入的目标文本(每句): 2042年，\n",
            "  2% 35/1500 [00:00<00:29, 50.31it/s]T2S Decoding EOS [169 -> 206]\n",
            "  2% 37/1500 [00:00<00:28, 51.90it/s]\n",
            "实际输入的目标文本(每句): 人工智能觉醒，\n",
            "  3% 42/1500 [00:01<00:38, 38.16it/s]T2S Decoding EOS [169 -> 213]\n",
            "  3% 44/1500 [00:01<00:37, 38.73it/s]\n",
            "实际输入的目标文本(每句): 世界各地纷纷沦陷。\n",
            "  3% 40/1500 [00:00<00:36, 40.08it/s]T2S Decoding EOS [169 -> 210]\n",
            "  3% 41/1500 [00:01<00:36, 39.58it/s]\n",
            "实际输入的目标文本(每句): 唯有铁一艾博机器人社团成为了世界最后的诺亚方舟，\n",
            "  7% 105/1500 [00:01<00:23, 60.32it/s]T2S Decoding EOS [169 -> 280]\n",
            "  7% 111/1500 [00:01<00:22, 60.96it/s]\n",
            "实际输入的目标文本(每句): 作为最后的人类，\n",
            "  3% 38/1500 [00:00<00:24, 59.46it/s]T2S Decoding EOS [169 -> 210]\n",
            "  3% 41/1500 [00:00<00:25, 58.15it/s]\n",
            "实际输入的目标文本(每句): 加入我们吧！\n",
            "  2% 26/1500 [00:00<00:24, 60.34it/s]T2S Decoding EOS [169 -> 198]\n",
            "  2% 29/1500 [00:00<00:25, 57.78it/s]\n",
            "实际输入的目标文本(每句): 艾博机器人社宣。\n",
            "  3% 40/1500 [00:00<00:23, 62.01it/s]T2S Decoding EOS [169 -> 211]\n",
            "  3% 42/1500 [00:00<00:24, 59.18it/s]\n",
            "0.158\t10.296\t0.711\t0.652\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 0.0.0.0:9874 <> https://981881b4a749402bbc.gradio.live\n",
            "Killing tunnel 0.0.0.0:9871 <> https://c9527519f675e5f976.gradio.live\n",
            "Killing tunnel 0.0.0.0:9872 <> https://5c6b05e864245c2149.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the source and destination paths\n",
        "source_path = '/content/GPT-SoVITS/output'\n",
        "destination_path = '/content/drive/MyDrive/GPT-SoVITS/output'\n",
        "\n",
        "# Copy the directory recursively\n",
        "!cp -r {source_path} {destination_path}\n",
        "\n",
        "# Print a success message\n",
        "print('Successfully saved /content/GPT-SoVITS/output to Google Drive.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "yLujqSzf-lZU",
        "outputId": "6d92b3b5-eab4-417c-81c8-7e54095e6d50"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-be376df2a9b7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Mount your Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Specify the source and destination paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    127\u001b[0m   )\n\u001b[1;32m    128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the source and destination paths\n",
        "source_path = '/content/GPT-SoVITS/output/slicer_opt'\n",
        "destination_path = '/content/GPT-SoVITS/output/slicer_opt.zip'\n",
        "\n",
        "# Create the zip archive\n",
        "shutil.make_archive(destination_path, 'zip', source_path)\n",
        "\n",
        "# Print a success message\n",
        "print('Successfully compressed /content/GPT-SoVITS/output/slicer_opt to /content/GPT-SoVITS/output/slicer_opt.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFGNiTqMHDVP",
        "outputId": "b4a8fe2b-1434-488f-b89d-2feedd0234cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully compressed /content/GPT-SoVITS/output/slicer_opt to /content/GPT-SoVITS/output/slicer_opt.zip\n"
          ]
        }
      ]
    }
  ]
}